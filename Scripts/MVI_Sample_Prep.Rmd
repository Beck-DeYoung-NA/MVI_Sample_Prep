---
title: "MVI Sample Prep"
output: html_notebook
---
### Preparatory Steps

Consult `ReadMe.md` for instructions on how to configure the inputs
properly before running this script


Remember to update file paths and variables in `MVI_Config_and_helpers.R`

### Notes

-   **This code will output warnings and messages.** The warnings indicate
    a check is not met. These will be printouts with "Warning:" before
    them. If a check is passed, a message will be printed with "SUCCESS:"
    before it. If any warnings are printed, figure out why and report if
    the warning says to.
    
```{r warning=F, message=F}
if (!require("pacman")) install.packages("pacman")
# Loads in necessary packages, and installs them if you don't have them installed
pacman::p_load("tidyverse",
               "readxl",
               "lubridate",
               "gtools",
               "janitor")

source("MVI_Config_and_Helpers.R")

options(dplyr.summarise.inform = FALSE,
        scipen = 99999) # Get rid of scientific notation
```

### Loading in Raw Data

```{r Load in the data, message=F}
countries <- list.files(RAW_DATA_PATH) # Get list of countries in the Sampling Weighting Folder

var_mapping <- read_excel(HELPER_FILE_PATH, sheet = "Variable_Info") %>% # Variable naming info
  set_names(c("OG_Field", "New_Fieldname",	"Remove"))

  
country_codes <- read_excel(HELPER_FILE_PATH, sheet = "Country_Info") %>% # Country and language codes
  set_names(c("NA_Country_Code",	"Country",	"Country_Language",	"NA_Language_Index",
              "File_Ext",	"Language_Code",	"NA_Language_Code",	"CV_ICS_Region",	"Filename")) 

# Load in all raw data and join into a single dataframe
# COMMENT OUT LINES UNTIL AFTER save(...) ONCE THIS HAS ALREADY BEEN RUN ONCE

MVI_Sample <- map(countries, load_raw_data) %>%
  reduce(full_join) %>%
  mutate(across(c("Tenure", "Total_OD_Spend_USD"), as.numeric), # Convert to numeric
         NA_Cell_Number = as.numeric(Cell_Number))

save(MVI_Sample, file = f_str("../Temporary_Data/MVI_Sample_Combined_{MVIQ}.Rdata"))

# UNCOMMENT THIS IF YOU HAVE ALREADY CREATED MVI_SAMPLE BEFORE
# load(f_str("../Temporary_Data/MVI_Sample_Combined_{MVIQ}.Rdata"))
```

```{r Load in pml, message = FALSE}
# Load in PML
PML <- read_excel(HELPER_FILE_PATH,
                sheet = "PML_Info",
                skip = 1) %>% 
  # Convert certain columns to numeric
  mutate(across(c(NA_Country_Code, NA_Language_Index, File_Ext), as.numeric),
         NA_Cell_Number = as.numeric(Cell_Number),
         Subject_Line_Insertion =  if_else(Subject_Line_Insertion == "[No subject line insertion]", 
                                           NA_character_,
                                           # Remove non-breaking spaces at end of line
                                           gsub("[\u00A0\\h\\v]+$", "", Subject_Line_Insertion) %>%
                                           # Not sure if we care about these two
                                           # Change other non breaking spaces to regular spaces
                                           gsub("\u00A0", " ", .) %>% 
                                           # Remove zero-width spaces
                                           gsub("\u200B", "", .) %>% trimws())) %>%  
  filter(Sample_Requested > 0) %>% # Remove two Thailand P_codes that are not used anymore
  select(-Country) # The Country variable in the PML is not what we want

# Load in the desired reporting names for each product code
reporting_names <- read_excel(HELPER_FILE_PATH, sheet = "CV_Reporting_Names") %>% 
  set_names(c("NA_Product_Code", "CV_Reporting_Names", "Comment")) %>% 
  # Remove products who have been removed
  filter(is.na(Comment) | !str_detect(tolower(Comment), "removed")) %>% 
  select(-Comment) %>% 
  mutate(NA_Product_Code = as.character(NA_Product_Code))

# Add the reporting names to the PML
PML <- PML %>% left_join(reporting_names)
```

```{r Set up Summary Tables}
# Set up Cell Level summary table
# This is sent to OPs in a few chunks after wrong spends are added
cell_lvl_summary <- PML %>% 
  select(NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number, Sample_Requested) %>%
  #  Count the number of samples received for each market
  left_join(MVI_Sample %>% 
             count(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext,
                   name = "Sample_Recieved")) %>% 
  # Compute differences
  mutate(Dif = Sample_Recieved - Sample_Requested,
         Pct_dif = round(abs(Dif / Sample_Requested) * 100, 2))

# Create the market level summary table. This is added to at the end of the script
# Will be sent to OPs
mrkt_level_summary <- cell_lvl_summary %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Sample_Requested = sum(Sample_Requested, na.rm = T),
            Sample_Recieved = sum(Sample_Recieved, na.rm = T),
            Dif = sum(Dif, na.rm = T))
```

### Checking for Unique ID Duplicates

```{r}
# Check for duplicates within and across country codes
# This has never happened in the past, so if any warnings are shown, it means the raw data is wrong
dups_within_country_code <- MVI_Sample %>% 
  group_by(NA_Country_Code, Unique_Record_Identifier15) %>% 
  filter(n() > 1)

dups_across_country_code <- MVI_Sample[duplicated(MVI_Sample$Unique_Record_Identifier15),]

if (nrow(dups_within_country_code) > 0) {
  warning("Duplicate Unique Record ID's Found within country codes")
  warning("THIS IS A BIG DEAL AND MEANS SOMETHING IS WRONG WITH THE DATA")
  dups_within_country_code$Unique_Record_Identifier15 %>% unique() %>% print()
} else {
  message("SUCCESS: No duplciate Unique Record ID's Found within country codes")
}

if (nrow(dups_across_country_code) > 0) {
  warning("Duplicate Unique Record ID's Found across country codes")
  warning("THIS IS A BIG DEAL AND MEANS SOMETHING IS WRONG WITH THE DATA")
  dups_across_country_code$Unique_Record_Identifier15 %>% unique() %>% print()
} else {
  message("SUCCESS: No duplciate Unique Record ID's Found across country codes")
}
```

### Calculating Bad Spend Observations before deduplication

```{r Spend Checks Pre-Duplicates}
missing_spend_predup <- MVI_Sample %>% filter(is.na(Total_OD_Spend_USD)) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code, 
         NA_Language_Index, File_Ext, Cell_Number)

zero_spend_predup <- MVI_Sample %>% filter(Total_OD_Spend_USD == 0) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code,
         NA_Language_Index, File_Ext, Cell_Number)

neg_spend_predup <- MVI_Sample %>% filter(Total_OD_Spend_USD < 0) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code,
         NA_Language_Index, File_Ext, Cell_Number)
```

```{r}
# Bad spend summary table by cell number
bad_spends_predup <- MVI_Sample %>% 
  group_by(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext) %>% 
  summarize(
    noSpend = sum(is.na(Total_OD_Spend_USD)),
    zeroSpend = sum(Total_OD_Spend_USD == 0, na.rm = T),
    NegativeSpend = sum(Total_OD_Spend_USD < 0, na.rm = T)
  ) %>% 
  arrange(NA_Country_Code, NA_Language_Index, File_Ext) 

# Add bad spending to cell level summary
cell_lvl_summary <- cell_lvl_summary %>% left_join(bad_spends_predup)

cell_lvl_summary %>% write_csv(f_str("../Files_to_Send/Cell_Level_Summary_{MVIQ}.csv"), na = "")
```

```{r}
# Create a summary table of Spend in USD and LOC to send to OPs
spend_summary <- MVI_Sample %>%
  mutate(Total_OD_Spend_LOC = as.numeric(Total_OD_Spend_Local_Currency)) %>% 
  group_by(Country, NA_Country_Code) %>%
  summarize(across(
    c(Total_OD_Spend_USD, Total_OD_Spend_LOC),
    .fns = list(
      n_obs = ~length(.),
      n = ~sum(!is.na(.)),
      nmiss = ~sum(is.na(.)),
      mean = ~mean(., na.rm = TRUE),
      median = ~median(., na.rm = TRUE)
    )
  )) %>% ungroup() %>% 
  arrange(NA_Country_Code)

spend_summary %>% write_csv(f_str("../Files_to_Send/Spend_Summary_{MVIQ}.csv"), na = "")
```

### Tenure Check and Removal

```{r Tenure Removal}
tenure_to_remove <- MVI_Sample %>% 
  # 1 = "Australia, 39 = "New Zealand"
  filter((Tenure < 6 & !NA_Country_Code %in% c(1,39)) |
         (Tenure < 12 & NA_Country_Code %in% c(1,39)) |
          is.na(Tenure))

# Store in frequency format for comparison table
# This is added at the end of the script
wrong_tenure_freq <- tenure_to_remove %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Tenure_Removed = n())

# Remove wrong tenures
MVI_Sample <- MVI_Sample %>% 
  filter(!Unique_Record_Identifier15 %in% tenure_to_remove$Unique_Record_Identifier15)

if (nrow(tenure_to_remove) > 0){
  warning(f_str("There were {nrow(tenure_to_remove)} observations removed with erroneous tenure. These are found in the Diagnostic_Files/Removed_Tenure.csv file"))
  # Create the csv
  tenure_to_remove %>% write_csv(f_str("../Diagnostic_Files/Removed_Tenure_{MVIQ}.csv"), na = "")
  wrong_tenure_freq %>% write_csv(f_str("../Files_to_Send/Removed_Tenure_Summary_{MVIQ}.csv"), na = "")
} else{
  message("No observations removed because of erroneous tenure")
}
```


```{r}
# Adding PML data to the sample

# I use SuppressMessages here so it doesn't display the join output
suppressMessages(MVI_Sample <- MVI_Sample %>% left_join(PML))

# Checking cell numbers in the PML and sample
suppressMessages(cell_no_not_in_pml <- MVI_Sample %>% anti_join(PML) %>% 
  distinct(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext, NA_Cell_Number))

suppressMessages(cell_no_not_in_data <- PML %>% anti_join(MVI_Sample) %>% 
  distinct(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext, NA_Cell_Number))

if (nrow(cell_no_not_in_pml) > 0){
  warning("There are Cell Numbers in the sample data not found in the PML")
  cell_no_not_in_pml %>% print()
} else{
  message("SUCCESS: All Cell Numbers in the sample data found in the PML")
}

if (nrow(cell_no_not_in_data) > 0){
  warning("There are Cell Numbers in the PML not found in the sample data")
  cell_no_not_in_data %>% print()
} else{
  message("SUCCESS: All Cell Numbers in the PML found in the sample data")
}
```

### Variable Checks

```{r Variable Checks}
# Verify all values are as expected for each variable
var_checks <- MVI_Sample %>%
  summarise(
    Amex_ID =             all(nchar(Amex_Customer_ID) == 12),
    NA_Country_Code =     all(NA_Country_Code %in% 1:50),
    NA_Language_Index =   all(NA_Language_Index %in% 1:2),
    NA_Cell_Number =      all(NA_Cell_Number %in% 1:25),
    File_Ext =            all(File_Ext %in% c(NA, 1, 2)),
    NA_Product_Code =     all(NA_Product_Code %in% 101:5099),
    Product_Type =        all(Product_Type %in% c("RCP", "GRCC")),
    COB =                 all(COB %in% c("Y", "N")),
    Key_Prod =            all(Key_Prod %in% c(NA, "K")),
    Fee =                 all(Fee %in% c("Y", "N")),
    PML_Rewards =         all(PML_Rewards %in% c("Y", "N")),
    PML_CashBack =        all(PML_CashBack %in% c("Y", "N")),
    PML_MR =              all(PML_MR %in% c("Y", "N")),
    CV_Reporting_Names =  all(str_detect(CV_Reporting_Names, "^[\\w \\p{P}]+$")) # Can have any letters, digits, underscore, space, or punctuation mark
  ) %>% 
  gather(key = "Param", value = "Value") %>% 
  filter(!Value)

if (nrow(var_checks) > 0){
  warning("Variable found with wrong values:")
  message(var_checks$Param)
} else{
  message("SUCCESS: All variables are as expected")
}
```

```{r Missing Language}
# Only FR-CAN is missing so it can be changed to 2
# This is from SAS, the if statement will determine if another language has a missing language
# This was also originally done in Step 4, but it fits better up here
MVI_Sample <- MVI_Sample %>% 
  mutate(Language = if_else(is.na(Language) & Language_Code == "FR-CA", 
                            "2", Language))

missing_language <- MVI_Sample %>% 
  filter(is.na(Language)) %>% pull(Language_Code) %>% unique()

if (length(missing_language) > 0){
  warning(paste0("These Language codes have missing values: ",
                 paste(missing_language, collapse = ", ")))
}
```

### Misaligned PCT Codes (Old Step 2b)

```{r Load in PCT, message =F}
# Load in pct lists
pct_india <- read_csv(PCT_INDIA_PATH, col_types = 'cdcccc') %>% 
  set_names(c('Cell_Number', 'NA_Country_Code', 'Product_Code', 'Formatted_PCT_Code', 'Card_ProductAIF', 'COUNTRY'))
pct_ex_india <- read_csv(PCT_EX_INDIA_PATH, col_types = 'cdcccccc') %>% 
  set_names(c('Cell_Number', 'NA_Country_Code', 'Product_Code', 'Formatted_PCT_Code', 
              'Card_ProductAIF', 'COUNTRY', 'NA_Prd_Code'))
```

```{r Misaligned PCTs}
# Combine the India and non-India lists and then reformat some variables
aif_pct <- pct_india %>% bind_rows(pct_ex_india) %>% 
  mutate(Formatted_PCT_Code = substr(Formatted_PCT_Code, 1, 3),
         Cell_Number = str_pad(Cell_Number, 2, pad = "0"),
         NA_Prd_Code = str_pad(NA_Prd_Code, 4, pad = "0")) %>% distinct() # Remove duplicates

just_prod_codes <- aif_pct %>% distinct(Cell_Number, NA_Country_Code, NA_Prd_Code)

# Check Product Code
MVI_with_prod_codes <- MVI_Sample %>% 
  select(Unique_Record_Identifier15, Cell_Number, NA_Country_Code, NA_Language_Index, NA_Product_Code, Product_Code) %>% 
  mutate(NA_Product_Code = str_pad(NA_Product_Code, 4, pad = "0")) %>% 
  left_join(just_prod_codes, by = c("Cell_Number", "NA_Country_Code")) %>% 
  mutate(dif = NA_Product_Code == NA_Prd_Code)

na_prod_code_misalignment <- MVI_with_prod_codes %>% filter(!dif)

if (nrow(na_prod_code_misalignment) > 0){
  warning("There is NA Product Code misalignment with AIF")
  warning("Check the PCT_Codes_Not_in_AIF.csv ")
  
} else{
  message("SUCCESS: No NA Product Code misalignment with AIF")
}

misaligned_pct_codes <- MVI_with_prod_codes %>% 
  select(Unique_Record_Identifier15, Cell_Number, NA_Country_Code, 
         NA_Language_Index, NA_Product_Code, Product_Code) %>% 
  arrange(NA_Country_Code, Cell_Number, Product_Code) %>% 
  anti_join(aif_pct, by = c("Cell_Number", "NA_Country_Code", "Product_Code" = "Formatted_PCT_Code"))

n_misaligned <- nrow(misaligned_pct_codes)
if (n_misaligned > 0){
  warning(f_str("There are {n_misaligned} observations with PCT Code misaligned with AIF"))
  
  misaligned_pct_codes %>% write_csv(f_str("../Files_to_Send/PCT_not_in_AIF_{MVIQ}.csv"), na = "")
  
} else{
  message("SUCCESS: No PCT Code misalignment with AIF")
}
```

```{r }
# There's probably a better way to display this information than the current system
# Summarize misalignment
misaligned_pct_codes_sum <- misaligned_pct_codes %>% select(-Unique_Record_Identifier15) %>% 
  group_by(NA_Country_Code, NA_Language_Index,Cell_Number, Product_Code) %>% 
  summarize(Count = n()) %>% ungroup()

# Get every pct code for a given NA product code
pct_code_spread <- aif_pct %>% select(-Product_Code) %>% 
  group_by(NA_Country_Code, Cell_Number, NA_Prd_Code) %>% 
  mutate(Card_ProductAIF = paste(unique(Card_ProductAIF), collapse = "/")) %>% 
  ungroup() %>% 
  group_by(NA_Country_Code, 
           Cell_Number,
           NA_Prd_Code,
           Card_ProductAIF) %>% 
  mutate(ord = paste0("PCT_Code_", row_number())) %>% 
  ungroup() %>% 
  spread(key = ord, value = Formatted_PCT_Code)

# Get the variable names (PCT_Code_1 - PCT_Code_77) for the purpose of ordering
pcts <- names(pct_code_spread)[str_detect(names(pct_code_spread), "PCT_Code_[0-9]")]

# Append AIF pct codes to misaligned summary table
misaligned_pct_codes_sum <- misaligned_pct_codes_sum %>% 
  left_join(pct_code_spread) %>% 
  select(NA_Country_Code, Cell_Number, 
         "PML NA_Product_Code" = NA_Prd_Code,
         Sample_PCT_Code = Product_Code,
         Card_ProductAIF, Count,
         mixedsort(pcts)) # This orders the pct_codes properly

# Write summary file
misaligned_pct_codes_sum %>%
  write_csv(f_str("../Files_to_Send/PCT_not_in_AIF_Summary_{MVIQ}.csv"), na = "")
```

### STOP HERE: SEND FILES TO OPS {style="color: red"}

Send the following files to OPS
  - `Cell_Level_Summary.csv`
  - `Spend_Summary.csv`
  - `Removed_Tenure_Summary.csv`
  - `Removed_Tenure.csv`
  - `PCT_not_in_AIF_Summary.csv`
  - `PCT_not_in_AIF.csv`
  
But, instead of attaching files, save the files in the `\\pm1\33-626\Quantitative\Sampling-Weighting\Communications\QUARTER` folder and send the links. Here is an example email.

    Hi Allie,
    
    Here are some initial findings from the data for you to review. Please let us know what you think.
    
      - Summary of sample received vs sample requested by cell number: \\pm1\33-626\Quantitative\Sampling-Weighting\Communications\Q3\Cell_Level_Summary_MVIQ323.csv
              - All markets line up with the total sample requested and received
              - Nothing appears to be too alarming in the cell level differences, except possibly for Cell Number 1 in Market 1 where the Sample Requested was 2233 and the Sample Received was 1735, a 22% difference. 
    
    -  Spend summary statistics prior to removing any observations: \\pm1\33-626\Quantitative\Sampling-Weighting\Communications\Q3\Spend_Summary_MVIQ323.csv
    
    Here are summary statistics of the people that were removed for low tenure, which is <12 in Australia and New Zealand and <6 elsewhere. There were 538 people removed, all from Belgium.
    -   \\pm1\33-626\Quantitative\Sampling-Weighting\Communications\Q3\Removed_Tenure_Summary_MVIQ323.csv
    - If you want the individual people who were removed: \\pm1\33-626\Quantitative\Sampling-Weighting\Communications\Q3\Removed_Tenure_MVIQ323.csv
    
    THIS IS WHERE WE NEED ACTION TO MOVE ON TO THE NEXT STEPS.
    Finally, here are a the PCT Codes from the PML that do not line up with the AIF (based on country code, cell number, and NA product code): \\pm1\33-626\Quantitative\Sampling-Weighting\Communications\Q3\PCT_not_in_AIF_Summary_MVIQ323.csv
    -	The column “Sample_PCT_Code” is the code from the sample data that was not found in the AIF file. The columns PCT_Code_1 through PCT_Code_80 are the PCT codes in the AIF file for the given NA Product Code.
    -	Let us know what you want us to do with the observations with these wrong PCT codes (remove, change, or something else).
    -	The non-summary file has the individual customers that are misaligned: \\pm1\33-626\Quantitative\Sampling-Weighting\Communications\Q3\PCT_not_in_AIF_MVIQ323.csv
    
    Let me know if you have any questions.

#### **When PM gets back on which to remove, update the next chunk**

```{r}
# Default to removing all
pct_codes_to_remove <- misaligned_pct_codes 

# In the following, add filter statements to remove PCT codes that the PM said we should not delete
# The pct_codes_to_remove should only include what we want to remove

# pct_codes_to_remove <- pct_codes_to_remove %>% 
#   filter(!(NA_Country_Code == X & Cell_Number == Y & Product_Code == Z)) %>%
#   filter(!(NA_Country_Code == X & Cell_Number == Y & Product_Code == Z)) %>%

n_before <- nrow(MVI_Sample)

MVI_Sample <- MVI_Sample %>% 
  anti_join(pct_codes_to_remove, by = "Unique_Record_Identifier15")

message(f_str("There were {n_before - nrow(MVI_Sample)} samples removed with erroneous PCT Codes"))

# For Summary table at the end
removed_pcts_ct <- pct_codes_to_remove %>% 
  group_by(NA_Country_Code, NA_Language_Index) %>% 
  summarize(Misaligned_PCTs = n())
```
```{r}
rm(MVI_with_prod_codes)
```

### Duplicate Removal

```{r}
amex_id_dups <- MVI_Sample %>% 
  group_by(Amex_Customer_ID) %>% 
  filter(n() > 1) 

amex_id_dups_within_country <- MVI_Sample %>% 
  group_by(NA_Country_Code, Amex_Customer_ID) %>% # CHANGED TO BE ACROSS COUNTRY
  filter(n() > 1) 

amex_id_dups_across_country <- amex_id_dups %>% anti_join(amex_id_dups_within_country, by = "Unique_Record_Identifier15")

n_dups <- n_distinct(amex_id_dups$Amex_Customer_ID)
n_dups_across <- n_distinct(amex_id_dups_across_country$Amex_Customer_ID)

message(f_str("There are {n_dups} duplicate Amex Customer IDs, leading to {nrow(amex_id_dups) - n_dups} records being removed."))

if (n_dups_across > 0){
  warning(f_str("Of these, there are {n_dups_across} duplicate Amex Customer IDs across countries, with {nrow(amex_id_dups_across_country)} total duplicate records. Please consult the PM about what to do about these."))
  
  amex_id_dups_across_country %>% 
    select(NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number, Amex_Customer_ID, Unique_Record_Identifier15, Tenure, Total_OD_Spend_USD) %>% 
    write_csv("../Files_to_Send/Amex_ID_dups_across_country.csv")
}
```


```{r}
amex_dups_summary <- amex_id_dups %>% 
  freq_table(c("NA_Country_Code", "NA_Language_Index", "File_Ext"))

amex_dups_summary %>% write_csv(f_str("../Files_to_Send/Amex_ID_Dups_Summary_{MVIQ}.csv"), na = "")
amex_id_dups %>% write.csv(f_str('../Diagnostic_Files/Amex_ID_Dups_{MVIQ}.csv'), na = "")
```

```{r}
# Sort data by na_cell_number in order to keep the dup record with the higher priority - 
  # in this case, the number closest to 1. i.e 1>2>3>4>5...;
# If there are DUPes with the same cell number, just keep any one record - doesn't matter b/c spend is the same for all dupes;
MVI_Sample_no_dups <-  MVI_Sample %>% 
  arrange(NA_Country_Code, Cell_Number) %>% 
  group_by(NA_Country_Code, Amex_Customer_ID) %>% 
  filter(row_number() == 1) %>% ungroup()
```

```{r}
########### DUPLICATES ACROSS COUNTRY
# THIS IS NOT SET IN STONE
# COULD BE WE WANT THE ONES THAT DONT HAVE ENOUGH SAMPLE

 MVI_Sample_no_dups <- MVI_Sample_no_dups %>% 
  # We want the first occurrence by cell number
  arrange(Cell_Number) %>% 
  group_by(Amex_Customer_ID) %>% 
  filter(row_number() == 1) %>% ungroup() %>% 
  # Reset ordering
  arrange(NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number)
```

```{r Dups for Comp Table}
removed_dups <- anti_join(amex_id_dups, MVI_Sample_no_dups, by = "Unique_Record_Identifier15")

removed_dups_summary <- removed_dups %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Amex_ID_dupes = n())
```

```{r}
# Clear some memory
rm(MVI_Sample)
```


### Creating New Variables


```{r Create new variables, message=F}
# Load in new variable rules
cv_product_codes <- read_excel(HELPER_FILE_PATH, sheet = "CV_Product_Codes")

# Create a table of product codes
cv_vars_tab <- MVI_Sample_no_dups %>% distinct(NA_Product_Code)

# Append the new variables to the product code table based on the PM's instructions
for (var in CV_VARS) cv_vars_tab <- cv_vars_tab %>% add_cv_var(var, cv_product_codes)

# Apply the new variables to sample
MVI_Sample_new_vars <- MVI_Sample_no_dups %>% left_join(cv_vars_tab)
```

```{r Create new variables 2}
MVI_Sample_new_vars <- MVI_Sample_new_vars %>% 
  mutate(
    CV_COBRAND_AIRLINES = case_when(
    	NA_Product_Code %in% c('3556', '3570') ~ "",
    	CV_COBRAND == "TRUE" & CV_AIRLINE == "TRUE" ~ "Yes",
    	CV_COBRAND == "TRUE" & CV_AIRLINE == "FALSE" ~ "No",
    	TRUE ~ ""),
    
    CV_Key_Products = if_else(is.na(Key_Prod), "N", "Y"), # This was originally Key_Prod == "K" but it doesn't account for missing, and there's no other values anyway
    
    CV_Product_Type = case_when(
      Product_Type == "RCP" & COB == "N" ~ "RCP",
      Product_Type == "GRCC" & COB == "N" ~ "GRCC",
      COB == "Y" ~ "Co-Brand"),
    
    CV_PREMIUM = CV_CENTURION == "TRUE" | CV_PLATINUM == "TRUE",
    
    CV_INTERVIEW_DATE = paste0(YEAR, MONTH),
    
    CV_REPORTING_NAME = CV_Reporting_Names,
    
    Date_of_Birth = str_replace_all(Date_of_Birth, "-", "/"), # Some date are different format
    dYOB = as.Date(Date_of_Birth) %>% year(),
    
    GENERATION = case_when(is.na(dYOB) | 
                             dYOB < 1850 | dYOB > (YEAR - 17) ~ "",
                           dYOB <  1946 ~ "Silent: 1945 and prior",
                           dYOB <  1965 ~ "Baby Boomers: 1946 - 1964",
                           dYOB <  1980 ~ "Generation X: 1965 - 1979",
                           dYOB <  1989 ~ "Older Millennials: 1980 - 1988",
                           dYOB <  1997 ~ "Younger Millennials: 1989 - 1996",
                           dYOB >= 1997 ~ "Generation Z: 1997 and later"),
  Email = paste0(Unique_Record_Identifier15, "@amexgabmsurvey.com") # WHy are we doing this, when we do not include it in the output?
         )
```

#### Remove people with wrong member since or DOB

```{r Member Since and DOB flags}
# THIS MAY NEED TO BE CHANGED. IF IT'S QUARTER 1, WE SHOULD BE LOOKING AT Member_Since > (YEAR - 1)
member_since_flag <- MVI_Sample_new_vars %>% 
  filter(Member_Since < 1920 | Member_Since > YEAR)

dob_flag <- MVI_Sample_new_vars %>% 
  filter(dYOB < 1850 | dYOB > YEAR - 17)

# Remove flagged observations
MVI_Sample_new_vars <- MVI_Sample_new_vars %>% 
  filter(!Unique_Record_Identifier15 %in% 
           c(member_since_flag$Unique_Record_Identifier15, 
             dob_flag$Unique_Record_Identifier15))

# Get counts for summary table
ms_ct <- member_since_flag  %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Member_Since = n())

dob_ct <- dob_flag %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(YOB = n())

member_since_flag %>% 
  bind_rows(dob_flag) %>% 
  select(NA_Country_Code, NA_Language_Index, File_Ext, Amex_Customer_ID, Unique_Record_Identifier15, Member_Since, dYOB, Tenure) %>% 
  write_csv(f_str("../Files_to_send/Member_Since_DOB_Flags_{MVIQ}.csv"), na = "")
```


```{r Final Sample Counts}
final_counts <- MVI_Sample_new_vars %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Final_Sample = n())
```

```{r}
# Clear up memory
rm(MVI_Sample_no_dups)
```

### Weighting Segments

```{r Weighting Segments}
# Weighting Segments
# The weighting segment sheet doesn't include XS or XT, 
# So to make it easier, I'm just having one line for each segment with it's respective tenure and spend splits
# Then we create the 6 conditions for the various options given these splits
weighting_segments <- read_excel(HELPER_FILE_PATH, sheet = "Weighting_Segments") %>%
  filter(str_detect(WGT_Bucket, "MVI")) %>% # Removes SBS weights if they weren't removed in the helper file
  filter(str_detect(Tenure_Split, "\\+") & str_detect(Spend_Split, "\\+")) %>% # HTHS
  # Turn rows with multiple NA_Product_Codes into their own row to allow for joining with MVI data
  separate_rows(NA_Product_Code, sep = ", ") %>% 
  mutate(NA_Product_Code = trimws(NA_Product_Code), # just in case there were extra spaces
         pc_cond = paste0("NA_Product_Code == ", NA_Product_Code),
         # Turn {QUARTER}-9{SEGMENT} into {QUARTER}{SEGMENT}
         WV_Weighting_Segment = str_remove(WGT_Bucket, "-[0-9]"), 
         Tenure_Split = parse_number(Tenure_Split),
         Spend_Split = parse_number(Spend_Split) * 1000,
         )
  
MVI_Sample_ws <- MVI_Sample_new_vars %>% left_join(weighting_segments) %>% 
  mutate(Tenure_Bucket = case_when(Tenure >= 6 & Tenure < Tenure_Split ~ "LT",
                                   Tenure >= Tenure_Split ~ "HT",
                                   TRUE ~ "XT"), # This can't occur as Tenure <6 is removed above
         Spend_Bucket  = case_when(Total_OD_Spend_USD > 0 & Total_OD_Spend_USD < Spend_Split ~ "LS",
                                   Total_OD_Spend_USD >= Spend_Split ~ "HS",
                                   TRUE ~ "XS"),
         Weighting_Segment = if_else(Spend_Bucket == "XS",
                                     "MVINOSPEND",
                                     paste0(WV_Weighting_Segment, Tenure_Bucket, Spend_Bucket))) %>% 
  select(-ends_with("Bucket"), everything())
```

```{r}
# Table showing observed minimums and maximums for tenure and spend for each weighting segment
ws_table <- MVI_Sample_ws %>% group_by(Weighting_Segment, Tenure_Split, Spend_Split) %>%  
  summarize(min_t = min(Tenure),
            max_t = max(Tenure),
            min_s = min(Total_OD_Spend_USD),
            max_s = max(Total_OD_Spend_USD)) %>% 
  make_nice_table("Weighting Segments")
```

```{r}
# Clear up memory
rm(MVI_Sample_new_vars)
```

```{r Scientific Notation}
# Convert product codes with 2nd character as E and 1st and 3rd characters 0-9. We need to create a temp product code variable which recodes the 2rd character from E to # in order to prevent excel from changing the product code to scientific notation. must recode the # back to E in excel;

# CAN WE REMOVE THIS BECAUSE WE ARE DIRECTLY WRITING TO CSVs AND NOT USING EXCEL
sci_no_to_fix <- MVI_Sample_ws %>% 
  distinct(NA_Country_Code, Product_Code) %>% 
  filter(str_detect(Product_Code, "[0-9]E[0-9]"))

MVI_Sample_ws <- MVI_Sample_ws %>% 
  mutate(Product_Code = ifelse(str_detect(Product_Code, "[0-9]E[0-9]"),
                               gsub("E", "#", Product_Code),
                               Product_Code))
```

### Reformatting to match what Qualtrics is expecting

```{r}
# Change all date columns to "M/D/YYYY" format
MVI_Sample_ws <- MVI_Sample_ws %>% 
  mutate(across(c("Establishment_Date"), ~str_replace_all(., "-", "/")), # Some date are different format
         across(c("Establishment_Date"), ~format(as.Date(.x), format = "%m/%d/%Y")),
         # We need to remove leading zeros in months and days
         across(c("Establishment_Date"), ~gsub("^0", "", gsub("/0", "/", .))),
         # Make Customer ID 15 digits
         Amex_Customer_ID = str_pad(Amex_Customer_ID, 15, "left", "0"),
         # Make Country Code 2 digits
         NA_Country_Code = str_pad(NA_Country_Code, 2, "left", "0"),
         # Make Product Code 4 digits
         NA_Product_Code = str_pad(NA_Product_Code, 4, "left", "0")
         )
```

#### Calculating Bad Spend Observations after Deduplication

```{r Spend Checks After-Duplicates}
missing_spend <- MVI_Sample_ws %>% filter(is.na(Total_OD_Spend_USD)) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number)

zero_spend <- MVI_Sample_ws %>% filter(Total_OD_Spend_USD == 0) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number)

neg_spend <- MVI_Sample_ws %>% filter(Total_OD_Spend_USD < 0) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number)

# Write to csv
missing_spend %>% write_csv(f_str("../Diagnostic_Files/Missing_Spend_{MVIQ}.csv"), na = "")
zero_spend %>% write_csv(f_str("../Diagnostic_Files/Zero_Spend_{MVIQ}.csv"), na = "")
neg_spend %>% write_csv(f_str("../Diagnostic_Files/Negative_Spend_{MVIQ}.csv"), na = "")
```

```{r}
# Bad spend summary table by cell number
bad_spends <- MVI_Sample_ws %>% 
  group_by(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext) %>% 
  summarize(
    noSpend = sum(is.na(Total_OD_Spend_USD)),
    zeroSpend = sum(Total_OD_Spend_USD == 0, na.rm = T),
    NegativeSpend = sum(Total_OD_Spend_USD < 0, na.rm = T)
  ) %>% 
  arrange(NA_Country_Code, NA_Language_Index, File_Ext) %>% ungroup()

bad_spends_countrylevel <- bad_spends %>% 
  select(-Cell_Number) %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(across(.fns = ~sum(., na.rm = T)))
```

#### Create Market Level Summary Table

```{r}
mrkt_level_summary <- mrkt_level_summary %>% 
  left_join(final_counts) %>% 
  left_join(dob_ct) %>% 
  left_join(ms_ct) %>% 
  left_join(wrong_tenure_freq) %>% 
  left_join(removed_pcts_ct) %>%
  left_join(removed_dups_summary) %>% 
  left_join(bad_spends_countrylevel) %>% 
  adorn_totals('row')

mrkt_level_summary %>% write_csv(f_str("../Files_to_send/Market_Level_Summary_{MVIQ}.csv"), na = "")
```

### Creating Final Sample

```{r Create Final Sample}
# Add AIF PCT Codes (Old Step 5)
MVI_final_postals <- MVI_Sample_ws %>% left_join(aif_pct %>% mutate(aif = TRUE, 
                                                                    NA_Country_Code = str_pad(NA_Country_Code, 2, "left", "0"))) %>% 
  mutate(Card_Product = if_else(is.na(aif), Card_Product, Card_ProductAIF), # Products not in the AIF get to keep their names
         # These two variables are just blanks
         MYCA_Indicator = "",
         Paperless_Indicator = ""
         ) 
MVI_final <- MVI_final_postals %>% 
  select(Cell_Number,
         COUNTRY_CODE = NA_Country_Code,
         Product_Code,
         Member_Since,
         Establishment_Date,
         Tenure,
         Language_Preference_Code = Language,
         MYCA_Indicator, 
         Paperless_Indicator,
         MR_Member_ID,
         Card_Product, 
         Account_number_last_5_digits,
         Amex_Customer_ID,
         Avg_Overseas_Domestic_ROCs,
         MR_Points,               
         Tot_Overseas_Domest_Spend_LOC = Total_OD_Spend_Local_Currency,
         Tot_Overseas_Domest_Spend_USD = Total_OD_Spend_USD,
         UID = Unique_Record_Identifier15,
         COUNTRY = Country,
         NAXION_PRODUCT_CODE = NA_Product_Code,
         CV_ICS_Region,
         CV_Product_Type,
         CV_Key_Products,
         Fee,
         CV_PML_Rewards = PML_Rewards,
         CV_PML_CashBack = PML_CashBack,
         CV_PML_FYF,
         CV_PML_WAIVER,
         CV_PML_MR = PML_MR,
         CV_CENTURION,
         CV_PLATINUM,
         CV_PREMIUM,
         CV_REPORTING_NAME,
         CV_COBRAND_AIRLINES,
         CV_RCP_GOLD,
         CV_RCP_GREEN,
         CV_INTERVIEW_DATE,
         LANGUAGE_CODE = Language_Code,
         WV_WEIGHTING_SEGMENT = Weighting_Segment,
         SUBJECT_LINE_INSERTION = Subject_Line_Insertion, 
         GENERATION,
         
         # Remove after checking
         File_Ext,
         Date_of_Birth,
         CV_COBRAND,
         CV_AIRLINE,
         COB,  
         Filename
         )
```

```{r}
# Clear up memory
rm(MVI_Sample_ws)
```

```{r Writing Merged File with Checks}
# NOT SURE IF THIS IS NECESSARY TO SEND. THIS IS SO OPS CAN VERIFY WHAT WAS DONE INTERNALLY
# This includes extra variables to allow for checking
         # File_Ext,
         # Date_of_Birth,
         # CV_COBRAND,
         # CV_AIRLINE,
         # COB,  
         # Filename
MVI_final %>% write_csv(f_str("../All_Sample_Files/MVI_Final_Checking_{MVIQ}.csv"), na = "")
save(MVI_final, file="../Temporary_Data/MVI_Final_Checking.Rdata")
```

```{r}
# load("../Temporary_Data/MVI_Final_Checking.Rdata")
```

### STOP HERE: SEND FILES TO OPS {style="color: red"}

Send the following files to OPS
  - `Amex_ID_Dups_Summary.csv`
  - `Amex_ID_Dups.csv`
  - `Member_Since_DOB_Flags.csv`
  - `Removed_Tenure.csv`
  - `Market_Level_Summary.csv`

Once again, save copies of the files in the `Communications/Quarter` folder and send links. You also are sending the final csv for OPs and the PM to make their checks. 
  - `MVI_Final_Checking.csv`

Importantly, you need to create an excel workbook from the csv manually and send that. Open the csv, and reformat the numeric variables that need padded zeroes. Use the TEXT function in excel and use the following zeroes formats. DO NOT SAVE THE CSV. USE "SAVE AS" AND MAKE A NEW EXCEL FILE.
  - Cell_Number : "00" 
  - COUNTRY_CODE : "00"
  - Account_number_last_5_digits: "00000"
  - Amex_Customer_ID : "000000000000000"
  - NAXION_PRODUCT_CODE: "0000"
  

Importantly, the non-english characters in Subject_Line_Insertion will still be messed up in the excel. This is okay, since they are never changed in the csv.
once this is done, copy the csv and xlsx into the `\\pm1\33-626\Quantitative\Sampling-Weighting\All Sample Files\QUARTER` folder
    
    Hi,
    
    There were 1276 duplicate Amex Customer IDs, leading to 1282 records being removed. 
    -	Herer is a summary by market: \\pm1\33-626\Quantitative\Sampling-Weighting\Communications\Q3\Amex_ID_Dups_Summary_MVIQ323.csv 
    -	The individual removals can be found here: \\pm1\33-626\Quantitative\Sampling-Weighting\Communications\Q3\Amex_ID_Dups_MVIQ323.csv
    -	There were no odd duplicates across countries like last quarter.
    
    Here is the list of people who were removed for having unlikely dates of birth or member_since values. Please confirm these are okay to be deleted. \\pm1\33-626\Quantitative\Sampling-Weighting\Communications\Q3\Member_Since_DOB_Flags_MVIQ323.csv
    -	There was one person removed with a member_since value of 9795: 
    -	There was one person removed with a date of birth in 2020
    
    Here is the final summary of all counts and removals that were done by market. It also includes the number of no spend, zero spend, and negative spend per market. 
    -	\\pm1\33-626\Quantitative\Sampling-Weighting\Communications\Q3\Market_Level_Summary_MVIQ323.csv
    
    
    Here are the final merged csv and excel for you to perform your checks. Excel messes up some formatting, but the csv contains all the true formatting of variables and is the file we use for the individual CSVs. For example, if you open the .xlsx file or if you open the .csv in excel, the Subject_Line_Insertion will look messed up for non-english characters. This is just an excel issue, the values are fine in the csv. You can double check them in the csv in a text editor like Notepad++. However, We reformatted numeric variables that need padded zeroes for your convenience.
    -	\\pm1\33-626\Quantitative\Sampling-Weighting\All Sample Files\Q3MVI_Final_Checking_MVIQ323.xlsx
    -	Here’s the csv: \\pm1\33-626\Quantitative\Sampling-Weighting\All Sample Files\Q3\MVI_Final_Checking_MVIQ323.csv
    -	Note there are a couple extra variables at the end to help with checking. These are removed later.
             # File_Ext,
             # Date_of_Birth,
             # CV_COBRAND,
             # CV_AIRLINE,
             # COB,  
             # Filename
    
    Please let me know if everything is all set to be written to individual market CSV's.


```{r Writing Merged File}
# Writing Official Merged File
# Get rid of checking variables
MVI_final <- MVI_final %>% 
  select(-c(File_Ext,
           Date_of_Birth,
           CV_COBRAND,
           CV_AIRLINE,
           COB))
         
MVI_final %>% select(-Filename) %>% write_csv(f_str("../All_Sample_Files/MVI_Final_{MVIQ}_{Sys.Date()}.csv"), na = "")
```

#### Write final files as individual CSVs

```{r Writing Market CSVs}
MVI_final %>%
  group_by(Filename) %>%
  # In group_walk, .x is the individual group and .y is the grouping name
  # So in our case, .x is the dataframe for a given filename, and .y is the filename
  group_walk(~write_csv(.x, file.path(MARKET_FILES_PATH, .y), na = ""))
```

### Postal Codes for Bob Crown

```{r}
postals <- MVI_final_postals %>% 
  select(Amex_Customer_ID,
         Unique_Record_Identifier15,
         NA_Product_Code,
         Country,
         Language_Code,
         Postal_Code)

postals %>% write_csv(f_str("../Files_to_send/Postal_Codes_{MVIQ}.csv"), na = "")
```
