---
title: "MVI Sample Prep"
output:
  html_document:
    df_print: paged
---
## Preparatory Steps

Consult `ReadMe.md` for instructions on how to configure the inputs
properly before running this script

Remember to update file paths and variables in `MVI_Config.R`

 **This code will output warnings and messages.** The warnings indicate
    a check is not met. These will be printouts with "Warning:" before
    them. If a check is passed, a message will be printed with a green "✔"
    before it. If any warnings are printed, figure out why and report if
    the warning says to. Many warnings will just be for typical removals.
    
```{r "Load Packages", warning=F, message=F}
if (!require("pacman")) install.packages("pacman")

# Loads in necessary packages, and installs them if you don't have them installed
pacman::p_load("tidyverse",
               "readxl",
               "lubridate",
               "gtools",
               "janitor",
               "openxlsx",
               "fs",
               "cli")

source("MVI_Helper_Functions.R") # Load in helper functions and config

options(dplyr.summarise.inform = FALSE,
        scipen = 99999) # Get rid of scientific notation

knitr::opts_chunk$set(results = "asis", # Prints html tables nicely
                      echo=TRUE) # Show all code chunks in the output

Diagnostic_WB <- createWorkbook() # Create Diagnostic workbook
```

## Loading in Raw Data

```{r "Load in the data"}
MVI_Sample <- load_MVI_raw_data(bypass_missing_folders = T)

# Save the raw sample data to a temporary folder so if you close R, you don't have to reload the raw data. Just load this file.
save(MVI_Sample, file = f_str("../Temporary_Data/{MVIQ}_Raw_Sample_{Sys.Date()}.Rdata"))

# UNCOMMENT THIS IF YOU HAVE ALREADY CREATED MVI_SAMPLE BEFORE AND COMMENT OUT THE LOADING AND SAVE LINES ABOVE
# LAST_DATE <- "2025-02-24"
# load(f_str("../Temporary_Data/{MVIQ}_Raw_Sample_{LAST_DATE}.Rdata"))

f_str("There were {format(nrow(MVI_Sample), big.mark = ',')} observations loaded") %>% print()
```

```{r "Load in pml", message = FALSE}
# Load in the desired reporting names for each product code
reporting_names <- load_reporting_names()

# Load in PML and add reporting names
PML <- load_pml() %>% left_join_suppress(reporting_names)
```


## SINGLE USE CODE EXAMPLES

### REMOVING CERTAIN PRODUCTS

This only needs to occur if samples were pulled for the given product but are no longer desired in the final sample. If the sample requested is 0, it's automatically ignored above. 

As an example:

For Q4 2023, The impacted products to be removed are: 
•	GRCC David Jones GPC (product code 0113)
•	GRCC David Jones Platinum (product code 0143)

```{r}
# to_remove <-  c(113, 143)
# 
# # Get PML products that we need to remove
# to_remove_PML <- PML %>% filter(NA_Product_Code %in% to_remove)
# 
# # Remove them from the PML
# PML <- PML %>% filter(!NA_Product_Code %in% to_remove)
# 
# # Identify samples to remove (and store there)
# to_remove_Sample <-  MVI_Sample %>% inner_join(to_remove_PML)
#
# # Remove from the Sample
# MVI_Sample <- MVI_Sample %>% anti_join(to_remove_Sample) 
```

### Updating PCT Codes that were misaligned

#### First - General Misalignment

This is misalignment that has occurred for multiple waves and can be fixed before doing the sample prep
```{r}
conditions_df <- data.frame(
  condition = c(
    'NA_Country_Code == 38 & NA_Cell_Number == 1 & Product_Code == "631"',
    'NA_Country_Code == 38 & NA_Cell_Number == 4 & Product_Code == "631"',
    'NA_Country_Code == 38 & NA_Cell_Number == 5 & Product_Code == "630"'
  ),
  new_product_code = c('A6I', 'EC', 'EB')
)

MVI_Sample <- MVI_Sample %>% update_and_count_product_codes(conditions_df)
```


#### Second - Quarter Specific Misalignment

NOTE YOU WILL ONLY KNOW IF YOU NEED TO DO THIS AFTER YOU SEND THE FIRST DIAGNOSTIC EMAIL HALFWAY DOWN THE SCRIPT. 

 - This occurs after we identified misalignment further down below, the code had to be adjusted up here so that we extracted the correct subject line inserts and card product names with the adjustments.
  - The information you need is the NA_Country_Code, NA_Cell_Number, NA_Language_Index, Product_Code, Update_NA_Cell, Update_Product, and or Update_NA_PCT_Code for all people to update.
 
```{r}
# pct_changes <- load_pct_changes()
# 
# MVI_Sample <- MVI_Sample %>%
#   left_join_suppress(pct_changes) %>%
#   mutate(Cell_Number = if_else(!is.na(Updated_Cell_Number), Updated_Cell_Number, Cell_Number),
#          NA_Cell_Number = if_else(!is.na(Updated_NA_Cell_Number), Updated_NA_Cell_Number, NA_Cell_Number),
#          Card_Product = if_else(!is.na(Updated_Card_Product), Updated_Card_Product, Card_Product),
#          Product_Code = if_else(!is.na(Updated_PCT_Code), Updated_PCT_Code, Product_Code)) %>%
#   select(-starts_with("Updated"), -NA_Product_Code)
```


### Summary tables

These are used for diagnostics and sent to OPS and PM later on

 - Scan though them and make sure the sample requested vs sample received is not drastically different. Or at least make sure the difference is at least consistent across all cells in the market.

```{r Set up Summary Tables}
# Set up Cell Level summary table
# This is sent to OPs in a few chunks
cell_lvl_summary <- PML %>% 
  select(NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number, Sample_Requested) %>%
  #  Count the number of samples received for each market
  left_join_suppress(MVI_Sample %>% 
             count(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext,
                   name = "Sample_Received")) %>% 
  # Compute differences
  mutate(Dif = Sample_Received - Sample_Requested,
         Pct_dif = round(abs(Dif / Sample_Requested) * 100, 2))

# Create the market level summary table. This is added to at the end of the script
# Will be sent to OPs at the end of the prep process
mrkt_level_summary <- cell_lvl_summary %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Sample_Requested = sum(Sample_Requested, na.rm = T),
            Sample_Received = sum(Sample_Received, na.rm = T),
            Dif = sum(Dif, na.rm = T))

# Add Market Level Summary to Diagnostic File
add_and_write_sheet(Diagnostic_WB, "Market_Level_Summary", mrkt_level_summary)
```

### Checking for Unique ID Duplicates

```{r}
# Check for duplicates IDs
# This has never happened in the past, so if any warnings are shown, it means the raw data is wrong

dups_ids <- MVI_Sample[duplicated(MVI_Sample$Unique_Record_Identifier15),]

log_section_start("Unique ID Duplicates")
log_check_result(
  condition = nrow(dups_ids) == 0,
  type = "Unique Record ID",
  check_type = "dupes",
  data = dups_ids,
  error_message = "Duplicate Unique Record ID's found. THIS IS A BIG DEAL AND MEANS SOMETHING IS WRONG WITH THE DATA"
)
```

### Calculating Bad Spend Observations Before Deduplication

```{r}
# Bad spend summary table by cell number
bad_spends_predup <- MVI_Sample %>% 
  group_by(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext) %>% 
  summarize(
    noSpend = sum(is.na(Total_OD_Spend_USD)),
    zeroSpend = sum(Total_OD_Spend_USD == 0, na.rm = T),
    NegativeSpend = sum(Total_OD_Spend_USD < 0, na.rm = T)
  ) %>% 
  arrange(NA_Country_Code, NA_Language_Index, File_Ext) 

# Add bad spending to cell level summary
cell_lvl_summary <- cell_lvl_summary %>% left_join_suppress(bad_spends_predup)

# Write to excel to eventually send to OPS Manager
add_and_write_sheet(Diagnostic_WB, "Cell_Level_Summary", cell_lvl_summary)
```

```{r}
# Create a summary table of Spend in USD and LOC to send to OPs
spend_summary <- MVI_Sample %>%
  mutate(Total_OD_Spend_LOC = as.numeric(Total_OD_Spend_Local_Currency)) %>% 
  group_by(Country, NA_Country_Code) %>%
  summarize(across(
    c(Total_OD_Spend_USD, Total_OD_Spend_LOC),
    .fns = list(
      n_obs =  ~length(.),
      n =      ~sum(!is.na(.)),
      nmiss =  ~sum(is.na(.)),
      mean =   ~mean(., na.rm = TRUE),
      median = ~median(., na.rm = TRUE)
    )
  )) %>% ungroup() %>% 
  arrange(NA_Country_Code)

# Write to excel to eventually send to OPS Manager
add_and_write_sheet(Diagnostic_WB, "Spend_Summary (Pre-Removals)", spend_summary)
```

### Tenure Check and Removal

```{r Tenure Removal}
to_remove_tenure <- MVI_Sample %>% 
  # 1 = "Australia, 39 = "New Zealand"
  filter((Tenure < 6 & !NA_Country_Code %in% c(1,39)) | # Tenure less than 6 not in AUS, NZ
         (Tenure < 12 & NA_Country_Code %in% c(1,39)) | # Tenure less than 12 in AUS, NZ
          is.na(Tenure))                                # Missing Tenure

# Store in frequency format for comparison table
# This is added at the end of the script
to_remove_tenure_summary <- to_remove_tenure %>% count(NA_Country_Code, NA_Language_Index, File_Ext, name = "Tenure_Removed") 

# Add tenure to summary table (just for diagnostic file)
to_remove_tenure_summary_w_tenure <- to_remove_tenure %>% 
  count(NA_Country_Code, NA_Language_Index, File_Ext, Tenure, name = "Tenure_Removed") %>% 
  arrange(NA_Country_Code, NA_Language_Index, File_Ext, Tenure)

# Log the results
log_check_result(
  condition = nrow(to_remove_tenure) == 0,
  type = "tenure",
  check_type = "validity",
  count = nrow(to_remove_tenure)
)

if(nrow(to_remove_tenure) > 0) {
  # Write to excel to eventually send to OPS Manager
  add_and_write_sheet(Diagnostic_WB, "Removed_Tenure_Individuals", to_remove_tenure)
  add_and_write_sheet(Diagnostic_WB, "Removed_Tenure_Summary", to_remove_tenure_summary_w_tenure)
}

# Remove wrong tenures
MVI_Sample <- MVI_Sample %>% 
  filter(!Unique_Record_Identifier15 %in% to_remove_tenure$Unique_Record_Identifier15)
```

```{r}
# Adding PML data to the sample
MVI_Sample <- MVI_Sample %>% left_join_suppress(PML)

# Checking cell numbers in the PML and sample
cell_no_not_in_pml <- MVI_Sample %>% anti_join_suppress(PML) %>% 
  distinct(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext)

cell_no_not_in_data <- PML %>% anti_join_suppress(MVI_Sample) %>% 
  distinct(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext)

log_section_start("Cells in PML vs Data")
log_check_result(
  condition = nrow(cell_no_not_in_pml) == 0,
  type = "cell numbers in sample data not in PML",
  check_type = "validity",
  data = cell_no_not_in_pml,
  success_message = "All Cell numbers in sample data are in the PML",
  row_message = "Country {row$NA_Country_Code} | Language {row$NA_Language_Index} | Cell {row$Cell_Number} | File Ext {row$File_Ext}"
)

log_check_result(
  condition = nrow(cell_no_not_in_data) == 0,
  type = "cell numbers in PML not in sample data",
  check_type = "validity",
  data = cell_no_not_in_data,
  success_message = "All Cell numbers in PML are in the sample data",
  row_message = "Country {row$NA_Country_Code} | Language {row$NA_Language_Index} | Cell {row$Cell_Number} | File Ext {row$File_Ext}"
)
```

### Variable Checks

- A common issue is PML_MR has an "Optional fee" value in Germany. Change this in the PML helper to "Y"

```{r Variable Checks}
# Verify all values are as expected for each variable
var_checks <- MVI_Sample %>%
  summarise(
    Amex_ID =             all(nchar(Amex_Customer_ID) == 12),
    NA_Country_Code =     all(NA_Country_Code %in% 1:50),
    NA_Language_Index =   all(NA_Language_Index %in% 1:2),
    NA_Cell_Number =      all(NA_Cell_Number %in% 1:25),
    File_Ext =            all(File_Ext %in% c(NA, 1, 2)),
    NA_Product_Code =     all(NA_Product_Code %in% 101:5099),
    Product_Type =        all(Product_Type %in% c("RCP", "GRCC")),
    COB =                 all(COB %in% c("Y", "N")),
    Key_Prod =            all(Key_Prod %in% c(NA, "K")),
    Fee =                 all(Fee %in% c("Y", "N")),
    PML_Rewards =         all(PML_Rewards %in% c("Y", "N")),
    PML_CashBack =        all(PML_CashBack %in% c("Y", "N")),
    PML_MR =              all(PML_MR %in% c("Y", "N")),
    CV_Reporting_Names =  all(str_detect(CV_Reporting_Names, "^[\\w \\p{P}]+$")) # Can have any letters, digits, underscore, space, or punctuation mark
  ) %>% 
  gather(key = "Param", value = "Value") %>% 
  filter(!Value)

log_check_result(
  condition = nrow(var_checks) == 0,
  type = "variable values",
  check_type = "validity",
  data = var_checks$Param
)
```

```{r Missing Language}
# Only FR-CAN is missing so it can be changed to 2
# This is from SAS, the if statement will determine if another language has a missing language
# This was also originally done in Step 4, but it fits better up here
MVI_Sample <- MVI_Sample %>% 
  mutate(Language = if_else(is.na(Language) & Language_Code == "FR-CA", 
                            "2", Language))

missing_language <- MVI_Sample %>% 
  filter(is.na(Language)) %>% pull(Language_Code) %>% unique()

log_check_result(
  condition = length(missing_language) == 0,
  type = "language code",
  check_type = "missing",
  data = missing_language
)
```

### Misaligned PCT Codes

Identifying PCT Codes that are misaligned with the AIF files

```{r Load in PCT}
# Load in pct list
aif_pct <- read_csv(PCT_LIST_PATH, col_types = 'ccccccn')  %>% 
  mutate(Formatted_PCT_Code = substr(Product_Code, 1, 3), # Some PCT Codes come in badly formatted with a bunch of extra characters
         Cell_Number = str_pad(Cell_Number, 2, pad = "0"),
         NA_Prd_Code = str_pad(NA_Prd_Code, 4, pad = "0"),
         NA_Country_Code = as.numeric(NA_Country_Code)) %>% 
  distinct()
```

```{r Misaligned PCTs}
# We first check misaligned NA Product Codes
aif_prod_codes <- aif_pct %>% distinct(Cell_Number, NA_Country_Code, NA_Prd_Code)

pct_code_spread <- aif_pct %>% 
  group_by(NA_Country_Code, Cell_Number, NA_Prd_Code) %>%
  mutate(Card_ProductAIF = paste(unique(Card_ProductAIF), collapse = "/")) %>% ungroup() %>%
  distinct(NA_Country_Code, Cell_Number, NA_Prd_Code, Card_ProductAIF)

# Append AIF Product Codes and Card Products to MVI Sample
MVI_with_prod_codes <- MVI_Sample %>% 
  select(Unique_Record_Identifier15, Cell_Number, NA_Country_Code, NA_Language_Index, NA_Product_Code, Product_Code, Card_Product) %>% 
  mutate(NA_Product_Code = str_pad(NA_Product_Code, 4, pad = "0")) %>% # Reformat so both files have same format
  left_join_suppress(pct_code_spread)

# Check if NA Product Codes are aligned with AIF
na_prod_code_misalignment <- MVI_with_prod_codes %>% filter(NA_Product_Code != NA_Prd_Code)

log_check_result(
  condition = nrow(na_prod_code_misalignment) == 0,
  type = "NA Product Codes",
  check_type = "validity",
  success_message = "No NA Product Code misalignment with AIF",
  error_message = "There is NA Product Code misalignment with AIF. I've never had this happen before, so nothing in place for it. Ask PM."
)

# Check PCT Codes
misaligned_pct_codes <- MVI_with_prod_codes %>% 
  select(Unique_Record_Identifier15, Cell_Number, NA_Country_Code, NA_Language_Index, 
         NA_Product_Code, Product_Code, Card_Product, Card_ProductAIF) %>% 
  # Only keep samples that are not in the aif, meaning they are misaligned
  anti_join(aif_pct, by = c("Cell_Number", "NA_Country_Code", "Product_Code" = "Formatted_PCT_Code"))  %>% 
  arrange(NA_Country_Code, Cell_Number, Product_Code) 


log_check_result(
  condition = nrow(misaligned_pct_codes) == 0,
  type = "PCT Codes",
  check_type = "counts",
  data = misaligned_pct_codes,
  row_message = "Country: {row$NA_Country_Code} | Cell: {row$Cell_Number} | NA Product: {row$NA_Product_Code} |> PCT_Code {row$Product_Code}",
)

# Add to the diagnostic file 
if (nrow(misaligned_pct_codes) > 0){
  
  add_and_write_sheet(Diagnostic_WB, "PCT_not_in_AIF_Individuals", misaligned_pct_codes)
  
  # Summarize
  misaligned_pct_code_sum <- misaligned_pct_codes %>% 
    count(NA_Country_Code, NA_Language_Index, Cell_Number, NA_Product_Code, Card_ProductAIF, Product_Code, name = 'Count')  %>% 
    mutate(Updated_NA_Cell_Number = NA,
           Updated_NA_Product_Code = NA,
           Updated_Card_Product = NA,
           Updated_PCT_Code = NA,
           )
  
  # Add to the diagnostic file
  add_and_write_sheet(Diagnostic_WB, "PCT_not_in_AIF_Summary", misaligned_pct_code_sum)
} 
```

```{r}
# Save Diagnostics file for Ops -- Not Final
saveWorkbook(Diagnostic_WB, file = f_str("../Files_to_send/{MVIQ}_Diagnostics-{Sys.Date()}.xlsx"), overwrite = TRUE)
```

### STOP HERE: SEND FILES TO OPS {style="color: red"}

Send the following file to OPS
  - `MVI_Diagnositcs_QUARTER.xlsx` found in the `Files_to_send` folder
  
But, instead of attaching files, copy the file to the `{PM_DRIVE}\Quantitative\Sampling-Weighting\Diagnostics\QUARTER` folder and send the link. Here is an example email.

    Hi,
    
    Here is a diagnostic file with some initial findings from the data for you to review. Please let us know what you think: {FILE PATH}.
    
    The file contains 7 sheets.
      - Market_Level_Summary: Summary of sample received vs sample requested by market
              - Nothing appears to be too alarming in the differences between requested and received samples. 
      - Cell_Level Summary: Summary of sample received vs sample requested by cell number
              - Nothing appears to be too alarming in the differences between requested and received samples. 
              - The largest percent differences occur in markets where all cells received the same amount of extra sample. For example, every cell in Belgium received 9 more samples than requested, which resulted in a 40% difference in one cell between requested and received.
    
    -  Spend_Summary (Pre-Removals): Spend summary statistics prior to removing any observations
    
    - Removed_Tenure_Individuals & Removed_Tenure_Summary: Individuals removed for low tenure, which is <12 in Australia and New Zealand and <6 elsewhere, and summary by market.
              - There were 529 people removed, all from Belgium (this has happened consistently in Belgium).
    
    THIS IS WHERE WE NEED ACTION TO MOVE ON TO THE NEXT STEPS.
    
    - PCT_not_in_AIF_Individuals & PCT_not_in_AIF_Summary: PCT Codes from the PML that do not line up with the AIF (based on country code, cell number, and NA product code)
        - There are only 4 instances this quarter.
        - We need action taken in the summary file.
        -	Let us know what you want us to do with the observations with these wrong PCT codes (remove, change, or something else).
          - Please update the empty columns in the summary sheet with the updates
    
    Let me know if you have any questions.

#### **When PM gets back on which to remove, update the next chunk**

  - If they say you can remove all samples then just proceed with running the next chunk
  
  
  - If they want you to update the values, then follow these steps
    1. Copy the sheet "PCT_not_in_AIF_Summary" to helper file and rename it to "PCT_Code_Changes"
    2. Go back up to the top of the code to the 'Load in the data' chunk, and just run the single line that loads in the MVI_Sample from the temporary data folder. This resets the data to the original state before we removed anything or made changes.
    3. Uncomment the code chunk under the header "Updating PCT Codes that were misaligned"
    4. Rerun everything under the loading in, and when you get to pct code misaligned, nothing should be misaligned anymore. 
    5. You do not need to resend the email of diagnostics. The diagnostic file will be updated, and will be sent at the end of the process. 
    6. Then run the chunk below. If all went well, `misaligned_pct_codes` will be empty and nothing will change to the database
      - We still need to run this chunk because we need the to_remove_pct_codes_summary for the summary at the end (it will just be zeros unless they wanted you to remove some on top of making changes)

```{r}
# Default to removing all
to_remove_pct_codes <- misaligned_pct_codes 

# In the following, add filter statements to remove PCT codes that the PM said we should NOT delete
# So the misaligned pct codes with country, cell, and product codes of (X1, Y1, Z1) and (X2, Y2, Z2) would not be removed in the below example. They are removed from the to_remove_pct_codes dataframe so they are not removed from the sample
# The to_remove_pct_codes should only include what we want to remove

# to_remove_pct_codes <- to_remove_pct_codes %>% 
#   filter(!(NA_Country_Code == X1 & Cell_Number == Y1 & Product_Code == Z1)) %>%
#   filter(!(NA_Country_Code == X2 & Cell_Number == Y2 & Product_Code == Z2)) %>%

n_before <- nrow(MVI_Sample)
MVI_Sample <- MVI_Sample %>% anti_join(to_remove_pct_codes, by = "Unique_Record_Identifier15")

cli_alert_info("There were {n_before - nrow(MVI_Sample)} samples removed with erroneous PCT Codes")

# For Summary table at the end
to_remove_pct_codes_summary <- to_remove_pct_codes %>% count(NA_Country_Code, NA_Language_Index, name = "Misaligned_PCTS")
```

```{r}
rm(MVI_with_prod_codes)
```

### Duplicate Removal

Identifying and removing duplicated AMEX IDs

```{r}
amex_id_dups <- MVI_Sample %>% 
  group_by(Amex_Customer_ID) %>% 
  filter(n() > 1) 

# For summary table at the end
amex_dups_summary <- amex_id_dups %>% freq_table(c("NA_Country_Code", "NA_Language_Index", "File_Ext"))

amex_id_dups_within_country <- MVI_Sample %>% 
  group_by(NA_Country_Code, Amex_Customer_ID) %>% 
  filter(n() > 1) 

amex_id_dups_across_country <- amex_id_dups %>% anti_join(amex_id_dups_within_country, by = "Unique_Record_Identifier15")

n_dups <- n_distinct(amex_id_dups$Amex_Customer_ID)
n_dups_across <- n_distinct(amex_id_dups_across_country$Amex_Customer_ID)

log_check_result(
  condition = n_dups == 0,
  type = "Amex Customer IDs",
  check_type = "dupes",
  count = n_dups,
  error_message = "There are {n_dups} duplicate Amex Customer IDs, leading to {nrow(amex_id_dups) - n_dups} records being removed. This is normal.",
)

log_check_result(
  condition = n_dups_across == 0,
  type = "Amex Customer IDs across countries",
  check_type = "dupes",
  count = n_dups_across,
  error_message = "There are {n_dups_across} duplicate Amex Customer IDs across countries, with {nrow(amex_id_dups_across_country)} total duplicate records. Please consult the PM about what to do about these."
)

if (nrow(amex_id_dups) > 0) {
  # Write to excel to send to OPS Manager
  add_and_write_sheet(Diagnostic_WB, "AMEXID_Dupes_Inds", amex_id_dups)
  add_and_write_sheet(Diagnostic_WB, "AMEXID_Dupes_Summary", amex_dups_summary)
}

if (nrow(amex_id_dups_across_country) > 0) {
  # Write to excel to send to OPS Manager
  add_and_write_sheet(Diagnostic_WB, "AMEXID_Dupes_Inds_Across_Country", amex_id_dups_across_country)
  
  # Summarize
  amex_dups_across_summary <- amex_id_dups_across_country %>% freq_table(c("NA_Country_Code", "NA_Language_Index", "File_Ext"))

  # Add to the diagnostic file
  add_and_write_sheet(Diagnostic_WB, "AMEXID_Dupes_Summary_Across_Country", amex_dups_across_summary)
}
```

```{r}
# Sort data by na_cell_number in order to keep the dup record with the higher priority - 
  # in this case, the number closest to 1. i.e 1>2>3>4>5...;
# If there are dupes with the same cell number, just keep any one record - doesn't matter b/c spend is the same for all dupes;
MVI_Sample_no_dups <-  MVI_Sample %>% 
  arrange(NA_Country_Code, Cell_Number) %>% 
  group_by(NA_Country_Code, Amex_Customer_ID) %>% 
  filter(row_number() == 1) %>% ungroup()
```

```{r}
########### DUPLICATES ACROSS COUNTRY
# THIS IS NOT SET IN STONE
# COULD BE WE WANT THE ONES THAT DONT HAVE ENOUGH SAMPLE
# THIS IS FOR THE PM/OPS TO DECIDE IF WE HAVE DUPLICATES ACROSS COUNTRY

 # MVI_Sample_no_dups <- MVI_Sample_no_dups %>% 
 #  # We want the first occurrence by cell number
 #  arrange(Cell_Number) %>% 
 #  group_by(Amex_Customer_ID) %>% 
 #  filter(row_number() == 1) %>% ungroup() %>% 
 #  # Reset ordering
 #  arrange(NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number)
```

```{r Removed Dupes}
to_remove_amex_id_dupes <- anti_join(amex_id_dups, MVI_Sample_no_dups, by = "Unique_Record_Identifier15")

to_remove_amex_id_dupes_summary <- to_remove_amex_id_dupes %>% 
  count(NA_Country_Code, NA_Language_Index, File_Ext, name = "Amex_ID_dupes")

# Write to excel to send to OPS Manager
add_and_write_sheet(Diagnostic_WB, "Rem_AMEXID_Dupes_Inds", to_remove_amex_id_dupes)
add_and_write_sheet(Diagnostic_WB, "Rem_AMEXID_Dupes_Summary", to_remove_amex_id_dupes_summary)
```

```{r}
# Clear some memory
rm(MVI_Sample)
```


### Creating New Variables


```{r Create new variables}
# Load in new variable rules
cv_product_codes <- load_cv_product_codes()

# Apply the new variables to sample
MVI_Sample_new_vars <- MVI_Sample_no_dups %>% add_cv_product_codes(cv_product_codes)
```

```{r Create new variables 2}
MVI_Sample_new_vars <- MVI_Sample_new_vars %>% left_join_suppress(aif_pct) %>% 
  mutate(
    # Products not in the AIF get to keep their names
    Card_Product = if_else(is.na(Card_ProductAIF), Card_Product, Card_ProductAIF), 
    
    CV_COBRAND_AIRLINES = case_when(
    	NA_Product_Code %in% c('3556', '3570') ~ "",
    	CV_COBRAND == "TRUE" & CV_AIRLINE == "TRUE" ~ "Yes",
    	CV_COBRAND == "TRUE" & CV_AIRLINE == "FALSE" ~ "No",
    	TRUE ~ ""),
    
    CV_Key_Products = if_else(is.na(Key_Prod), "N", "Y"), # This was originally Key_Prod == "K" but it doesn't account for missing, and there's no other values anyway
    
    CV_Product_Type = case_when(
      Product_Type == "RCP" & COB == "N" ~ "RCP",
      Product_Type == "GRCC" & COB == "N" ~ "GRCC",
      COB == "Y" ~ "Co-Brand"),
    
    CV_PREMIUM = CV_CENTURION == "TRUE" | CV_PLATINUM == "TRUE",
    
    CV_INTERVIEW_DATE = paste0(YEAR, MONTH),
    
    CV_REPORTING_NAME = CV_Reporting_Names,
    
    Date_of_Birth = str_replace_all(Date_of_Birth, "-", "/"), # Some date are different format
    dYOB = as.Date(Date_of_Birth) %>% year(),
    
    GENERATION = case_when(is.na(dYOB) | 
                             dYOB < 1850 | dYOB > (YEAR - 17) ~ "",
                           dYOB <  1946 ~ "Silent: 1945 and prior",
                           dYOB <  1965 ~ "Baby Boomers: 1946 - 1964",
                           dYOB <  1980 ~ "Generation X: 1965 - 1979",
                           dYOB <  1989 ~ "Older Millennials: 1980 - 1988",
                           dYOB <  1997 ~ "Younger Millennials: 1989 - 1996",
                           dYOB >= 1997 ~ "Generation Z: 1997 and later"),
    
    # These two variables are just blanks
     MYCA_Indicator = "",
     Paperless_Indicator = "",

    Email = paste0(Unique_Record_Identifier15, "@amexgabmsurvey.com") # Why are we doing this, when we do not include it in the output?
         )
```


#### Remove people with wrong member since or DOB

```{r Member Since Check}
# Check for Member Since
year_cond <- ifelse(Q == "Q1", YEAR - 1, YEAR) # IF IT'S QUARTER 1, WE SHOULD BE LOOKING AT Member_Since > (YEAR - 1)

to_remove_member_since <- MVI_Sample_new_vars %>% 
  filter(Member_Since < 1930 | Member_Since > year_cond) %>% arrange(Member_Since) %>% 
  select(NA_Country_Code, NA_Language_Index, File_Ext, Amex_Customer_ID, Unique_Record_Identifier15, Member_Since, dYOB, Tenure)

log_check_result(
  condition = nrow(to_remove_member_since) == 0,
  type = "Member Since",
  check_type = "counts",
  count = nrow(to_remove_member_since),
  error_message = "Found {count} invalid Member Since values. Earliest is {min(to_remove_member_since$Member_Since, na.rm = T)} and latest is {max(to_remove_member_since$Member_Since, na.rm = T)}."
)
  
# Get counts for summary table (For final counts, not diagnostic file)
to_remove_ms_summary <- to_remove_member_since %>% count(NA_Country_Code, NA_Language_Index, File_Ext, name = "Member_Since")

# Write to excel to send to OPS Manager
if (nrow(to_remove_member_since) > 0) add_and_write_sheet(Diagnostic_WB, "Removed_Member_Since", to_remove_member_since)
```


```{r Date of Birth Check}
# Check for Date of Birth
to_remove_dob <- MVI_Sample_new_vars %>% 
  filter(dYOB < 1850 | dYOB > YEAR - 17) %>% 
  select(NA_Country_Code, NA_Language_Index, File_Ext, Amex_Customer_ID, Unique_Record_Identifier15, Member_Since, dYOB, Tenure)

log_check_result(
  condition = nrow(to_remove_dob) == 0,
  type = "Date of Birth",
  check_type = "counts",
  count = nrow(to_remove_dob),
  error_message = "Found {count} invalid Date of Birth values. Earliest is {min(to_remove_dob$dYOB, na.rm = T)} and latest is {max(to_remove_dob$dYOB, na.rm = T)}."
)

# Get counts for summary table (For final counts, not diagnostic file)
to_remove_dob_summary <- to_remove_dob %>% count(NA_Country_Code, NA_Language_Index, File_Ext, name = "Date_of_Birth")

# Write to excel to send to OPS Manager
if (nrow(to_remove_dob) > 0) add_and_write_sheet(Diagnostic_WB, "Removed_DOB", to_remove_dob)
```

```{r}
# Remove flagged observations for Member Since and DOB
MVI_Sample_new_vars <- MVI_Sample_new_vars %>% 
  anti_join(to_remove_member_since, by = "Unique_Record_Identifier15") %>%
  anti_join(to_remove_dob, by = "Unique_Record_Identifier15") 
```


```{r Final Sample Counts}
# Final counts for market level summary
final_counts <- MVI_Sample_new_vars %>% count(NA_Country_Code, NA_Language_Index, File_Ext, name = "Final_Counts")
```

```{r}
# Clear up memory
rm(MVI_Sample_no_dups)
```

### Weighting Segments

```{r Weighting Segments}
# Load in weighting segments
weighting_segments <- load_weighting_segments()

# Apply weighting segments
MVI_Sample_ws <- MVI_Sample_new_vars %>% apply_weighting_segments(weighting_segments)
```

```{r}
# Table showing observed minimums and maximums for tenure and spend for each weighting segment
ws_table <- MVI_Sample_ws %>% group_by(Weighting_Segment, Tenure_Split, Spend_Split) %>%
  summarize(min_t = min(Tenure),
            max_t = max(Tenure),
            min_s = min(Total_OD_Spend_USD),
            max_s = max(Total_OD_Spend_USD)) %>%
  make_nice_table("Weighting Segments")
```

```{r}
# Clear up memory
rm(MVI_Sample_new_vars)
```

```{r Scientific Notation}
# Convert product codes with 2nd character as E and 1st and 3rd characters 0-9. We need to create a temp product code variable which recodes the 2rd character from E to # in order to prevent excel from changing the product code to scientific notation. must recode the # back to E in excel;

# CAN WE REMOVE THIS BECAUSE WE ARE DIRECTLY WRITING TO CSVs AND NOT USING EXCEL
sci_no_to_fix <- MVI_Sample_ws %>% 
  distinct(NA_Country_Code, Product_Code) %>% 
  filter(str_detect(Product_Code, "[0-9]E[0-9]"))

log_check_result(
  condition = nrow(sci_no_to_fix) == 0,
  type = "Product Codes",
  check_type = "validity",
  data = sci_no_to_fix,
  success_message = "No product codes with scientific notation",
  error_message = "There are {nrow(sci_no_to_fix)} product codes with scientific notation. This is not a problem, but it will cause issues when writing to excel. You will need to manually change '#' back to 'E' in excel.",
  row_message = "Country: {row$NA_Country_Code} | Product Code: {row$Product_Code}"
)

MVI_Sample_ws <- MVI_Sample_ws %>% 
  mutate(Product_Code = ifelse(str_detect(Product_Code, "[0-9]E[0-9]"),
                               gsub("E", "#", Product_Code),
                               Product_Code))
```


#### Calculating Bad Spend Observations after Deduplication

```{r}
# Bad spend summary table by cell number
bad_spends <- MVI_Sample_ws %>% 
  group_by(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext) %>% 
  summarize(
    noSpend = sum(is.na(Total_OD_Spend_USD)),
    zeroSpend = sum(Total_OD_Spend_USD == 0, na.rm = T),
    NegativeSpend = sum(Total_OD_Spend_USD < 0, na.rm = T)
  ) %>% 
  arrange(NA_Country_Code, NA_Language_Index, File_Ext) %>% ungroup()

bad_spends_countrylevel <- bad_spends %>% select(-Cell_Number) %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(across(everything(), .fns = ~sum(., na.rm = T)))
```

#### Create Market Level Summary Table

```{r}
# This contains all the summary tables we created through the code
mrkt_level_summary <- mrkt_level_summary %>% 
  left_join_suppress(final_counts) %>% 
  left_join_suppress(to_remove_dob_summary) %>% 
  left_join_suppress(to_remove_ms_summary) %>% 
  left_join_suppress(to_remove_tenure_summary) %>% 
  left_join_suppress(to_remove_pct_codes_summary) %>%
  left_join_suppress(to_remove_amex_id_dupes_summary) %>% 
  left_join_suppress(bad_spends_countrylevel) %>% 
  adorn_totals('row')

# Write to excel to send to OPS Manager
add_and_write_sheet(Diagnostic_WB, "Market_Level_Summary", mrkt_level_summary)
saveWorkbook(Diagnostic_WB, file = f_str("../Files_to_send/{MVIQ}_Diagnostics_Final_{Sys.Date()}.xlsx"), overwrite = TRUE)
```

### Reformatting to match what Qualtrics is expecting

```{r}
# Change all date columns to "M/D/YYYY" format
MVI_Sample_ws <- MVI_Sample_ws %>% 
  mutate(across(c("Establishment_Date"), ~str_replace_all(., "-", "/")), # Some date are different format
         across(c("Establishment_Date"), ~format(as.Date(.x), format = "%m/%d/%Y")),
         # We need to remove leading zeros in months and days
         across(c("Establishment_Date"), ~gsub("^0", "", gsub("/0", "/", .))),
         # Make Customer ID 15 digits
         Amex_Customer_ID = str_pad(Amex_Customer_ID, 15, "left", "0"),
         # Make Country Code 2 digits
         NA_Country_Code = str_pad(NA_Country_Code, 2, "left", "0"),
         # Make Product Code 4 digits
         NA_Product_Code = str_pad(NA_Product_Code, 4, "left", "0")
         )
```

### Saving Postal Codes for Bob Crown

```{r}
postals <- MVI_Sample_ws %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Product_Code, Country,
         Language_Code, Postal_Code)

postals %>% write_csv(f_str("../Files_to_send/{MVIQ}_Postal_Codes_{Sys.Date()}.csv"), na = "")
```

### Creating Final Sample

```{r Create Final Sample}
MVI_final <- MVI_Sample_ws %>% 
  select(Cell_Number,
         COUNTRY_CODE = NA_Country_Code,
         Product_Code,
         Member_Since,
         Establishment_Date,
         Tenure,
         Language_Preference_Code = Language,
         MYCA_Indicator, 
         Paperless_Indicator,
         MR_Member_ID,
         Card_Product, 
         Account_number_last_5_digits,
         Amex_Customer_ID,
         Avg_Overseas_Domestic_ROCs,
         MR_Points,               
         Tot_Overseas_Domest_Spend_LOC = Total_OD_Spend_Local_Currency,
         Tot_Overseas_Domest_Spend_USD = Total_OD_Spend_USD,
         UID = Unique_Record_Identifier15,
         COUNTRY = Country,
         NAXION_PRODUCT_CODE = NA_Product_Code,
         CV_ICS_Region,
         CV_Product_Type,
         CV_Key_Products,
         CV_Fee = Fee,
         CV_PML_Rewards = PML_Rewards,
         CV_PML_CashBack = PML_CashBack,
         CV_PML_FYF,
         CV_PML_WAIVER,
         CV_PML_MR = PML_MR,
         CV_CENTURION,
         CV_PLATINUM,
         CV_PREMIUM,
         CV_REPORTING_NAME,
         CV_COBRAND_AIRLINES,
         CV_RCP_GOLD,
         CV_RCP_GREEN,
         CV_INTERVIEW_DATE,
         LANGUAGE_CODE = Language_Code,
         WV_WEIGHTING_SEGMENT = Weighting_Segment,
         SUBJECT_LINE_INSERTION = Subject_Line_Insertion, 
         GENERATION,
         
         # Remove after checking
         File_Ext,
         Date_of_Birth,
         CV_COBRAND,
         CV_AIRLINE,
         COB,  
         Filename
         )
```

```{r}
# Save the checking db for quick changes after the fact
save(MVI_final, file=f_str("../Temporary_Data/{MVIQ}_Checking_{Sys.Date()}.Rdata"))
```

```{r Writing Merged File with Checks}
# THIS IS SO OPS CAN VERIFY WHAT WAS DONE INTERNALLY
# This includes extra variables to allow for checking
         # File_Ext,
         # Date_of_Birth,
         # CV_COBRAND,
         # CV_AIRLINE,
         # COB,  
         # Filename
MVI_final %>% write_csv(f_str("../All_Sample_Files/{MVIQ}_Checking_{Sys.Date()}.csv"), na = "")
```

```{r}
# This is if you wanted to close R while waiting for OPS to review everything
# You can just reload in the data without rerunning the whole script

last_save_date <- '2023-10-03' # last time you saved the mvi_final dataset
# load("../Temporary_Data/{MVIQ}_Checking_{last_save_date}.Rdata")
```

### STOP HERE: SEND FILES TO OPS {style="color: red"}

Send the following file to OPS (there are new sheets added)
  - `MVI_Diagnositcs_QUARTER.xlsx` found in the `Files_to_send` folder
  
Once again, save a copy of the file in the `Diagnostics/Quarter` folder and send a link. You can override the old file because the new file has the old data in it. You also are sending the final csv for OPs and the PM to make their checks. 
  - `MVI_Final_Checking-DATE.csv`

Importantly, you need to create an excel workbook from the csv manually and send that. Open the csv, and reformat the numeric variables that need padded zeroes. I usually insert a column at the start of the file (i.e., column A) and Use the TEXT function in excel and use the following zeroes formats. Then copy and paste AS VALUES into the respective columns to overwrite the original data. Once again. This is just for convenience for OPS, this does not affect the data in the official CSV or data. 

DO NOT SAVE THE CSV. USE "SAVE AS" AND MAKE A NEW EXCEL FILE. Also copy this new file to the communications folder

Formats: 
  - Cell_Number : TEXT(B2, REPT("0", 2)) 
  - COUNTRY_CODE : TEXT(C2, REPT("0", 2)) 
  - Account_number_last_5_digits: TEXT(M2, REPT("0", 5)) 
  - Amex_Customer_ID : TEXT(N2, REPT("0", 15)) 
  - NAXION_PRODUCT_CODE: TEXT(U2, REPT("0", 4)) 
  

Importantly, the non-english characters in Subject_Line_Insertion will still be messed up in the excel. This is okay, since they are never changed in the csv.

once this is done, copy the csv and xlsx into the `{PM_DRIVE}\Quantitative\Sampling-Weighting\All Sample Files\QUARTER` folder.

Here is an example email
    
    Hi,
    
    The final sample has been prepped. Please refer again to the diagnostic file (with new sheets added, and old ones updated): {FILE PATH}
    
    There were 6 sheets added.
      - AMEXID_Dupes_Ind & AMEXID_Dupes_Summary: Individuals and summary of all duplicate amex id observations
      - Rem_AMEXID_Dupes_Ind & Rem_AMEXID_Dupes_Summary: Individuals removed for having a duplicate Amex Customer ID. We kept the one observation with the highest priority. 
        - There were 1214 duplicate Amex Customer IDs, leading to 1221 records being removed.

      - Removed_Member_Since: People who were removed for having unlikely member_since values. 
        - 5 people were removed because of unlikey member_since values (>2023 or < 1930)
        
      - Removed_DOB: People who were removed for having unlikely dates of birth (Age <= 17 or DOB < 1850)
    
      - Market_lvl_Summary: Final summary of all counts and removals that were done by market. It also includes the number of no spend, zero spend, and negative spend per market.
    
    
    Here are the final merged csv and excel for you to perform your checks. Excel messes up some formatting, but the csv contains all the true formatting of variables and is the file we use for the individual CSVs. For example, if you open the .xlsx file or if you open the .csv in excel, the Subject_Line_Insertion will look messed up for non-english characters. This is just an excel issue, the values are fine in the csv. You can double check them in the csv in a text editor like Notepad++. However, We reformatted numeric variables that need padded zeroes for your convenience.
    -	{EXCEL FILE PATH}
    -	Here’s the csv: {CSV FILE PATH}
    -	Note there are a couple extra variables at the end to help with checking. These are removed later.
             # FileExt,
             # Date_of_Birth,
             # CV_COBRAND,
             # CV_AIRLINE,
             # COB,  
             # Filename
    
    Please let me know if everything is all set to be written to individual market CSV's.


```{r Writing Merged File}
# Writing Official Merged File
MVI_final <- MVI_final %>% 
  select(-c(File_Ext, Date_of_Birth, CV_COBRAND, CV_AIRLINE, COB)) # Get rid of checking variables
         
MVI_final %>% select(-Filename) %>% write_csv(f_str("../All_Sample_Files/{MVIQ}_Final_{Sys.Date()}.csv"), na = "")
```

#### Write final files as individual CSVs

```{r Writing Market CSVs}
MVI_final %>%
  group_by(Filename) %>%
  # In group_walk, .x is the individual group and .y is the grouping name
  # So in our case, .x is the dataframe for a given filename, and .y is the filename
  group_walk(~write_csv(.x, file.path(MARKET_FILES_PATH, .y), na = ""))
```


