---
title: "MVI Sample Prep"
output:
  html_document:
    df_print: paged
---
### Preparatory Steps

Consult `ReadMe.md` for instructions on how to configure the inputs
properly before running this script


Remember to update file paths and variables in `MVI_Config_and_helpers.R`

### Notes

-   **This code will output warnings and messages.** The warnings indicate
    a check is not met. These will be printouts with "Warning:" before
    them. If a check is passed, a message will be printed with "SUCCESS:"
    before it. If any warnings are printed, figure out why and report if
    the warning says to.
    
```{r warning=F, message=F}
if (!require("pacman")) install.packages("pacman")

# Loads in necessary packages, and installs them if you don't have them installed
pacman::p_load("tidyverse",
               "readxl",
               "lubridate",
               "gtools",
               "janitor",
               "openxlsx")

source("MVI_Config_and_Helpers.R")

options(dplyr.summarise.inform = FALSE,
        scipen = 99999) # Get rid of scientific notation

Diagnostic_WB <- createWorkbook() # Create Diagnostic workbook
```

### Loading in Raw Data

```{r Load in the data, message=F}
countries <- list.files(RAW_DATA_PATH) # Get list of countries in the Sampling Weighting Folder

var_mapping <- read_excel(HELPER_FILE_PATH, sheet = "Variable_Info") %>% # Variable naming info
  set_names(c("OG_Field", "New_Fieldname"))

  
country_codes <- read_excel(HELPER_FILE_PATH, sheet = "Country_Info") %>% # Country and language codes
  set_names(c("NA_Country_Code",	"Country",	"Country_Name_PML",	"Country_Name_CMS", "NA_Language_Index",
              "File_Ext",	"Language_Code",	"NA_Language_Code",	"CV_ICS_Region",	"Filename")) 

# Load in all raw data and join into a single dataframe
# COMMENT OUT LINES UNTIL AFTER save(...) ONCE THIS HAS ALREADY BEEN RUN ONCE

MVI_Sample <- map(countries, load_raw_data) %>%
  reduce(\(x, y) suppressMessages(full_join(x,y))) %>%
  mutate(across(c("Tenure", "Total_OD_Spend_USD"), as.numeric), # Convert to numeric
         NA_Cell_Number = as.numeric(Cell_Number))

save(MVI_Sample, file = f_str("../Temporary_Data/{MVIQ}_Sample_Combined_{Sys.Date()}.Rdata"))

# UNCOMMENT THIS IF YOU HAVE ALREADY CREATED MVI_SAMPLE BEFORE AND COMMENT OUT THE LOADING AND SAVE LINES ABOVE
# LAST_DATE <- "2024-06-19"
# load(f_str("../Temporary_Data/{MVIQ}_Sample_Combined_{LAST_DATE}.Rdata"))

f_str("There were {format(nrow(MVI_Sample), big.mark = ',')} observations loaded") %>% print()
```

```{r Load in pml, message = FALSE}
# Load in PML
PML <- read_excel(HELPER_FILE_PATH,
                sheet = "PML_Info",
                skip = 1) %>% 
  # Convert certain columns to numeric
  mutate(across(c(NA_Country_Code, NA_Language_Index, File_Ext, Sample_Requested), as.numeric),
         NA_Cell_Number = as.numeric(Cell_Number),
         # Fix subject_line_insertion variable
         Subject_Line_Insertion =  if_else(Subject_Line_Insertion == "[No subject line insertion]", 
                                           NA_character_,
                                           # Remove non-breaking spaces at end of line
                                           gsub("[\u00A0\\h\\v]+$", "", Subject_Line_Insertion) %>%
                                           # Not sure if we care about these two
                                           # Change other non breaking spaces to regular spaces
                                           gsub("\u00A0", " ", .) %>% 
                                           # Remove zero-width spaces
                                           gsub("\u200B", "", .) %>% trimws())) %>%  
  filter(Sample_Requested > 0) %>% # Remove two Thailand products that are not used anymore will also remove any 0 sample requested products
  select(-Country) # The Country variable in the PML is not what we want

# Load in the desired reporting names for each product code
reporting_names <- read_excel(HELPER_FILE_PATH, sheet = "CV_Reporting_Names") %>% 
  set_names(c("NA_Product_Code", "CV_Reporting_Names", "Comment")) %>% 
  # Remove products who have been removed
  filter(is.na(Comment) | !str_detect(tolower(Comment), "removed")) %>% 
  select(-Comment) %>% 
  mutate(NA_Product_Code = as.character(NA_Product_Code))

# Add the reporting names to the PML
PML <- PML %>% left_join(reporting_names)
```

# SINGLE USE CODE EXAMPLES

### REMOVING CERTAIN PRODUCTS

This only needs to occur if samples were pulled for the given product but are no longer desired in the final sample. If the sample requested is 0, it's automatically ignored above. 

As an example:

For Q4 2023, The impacted products to be removed are: 
•	GRCC David Jones GPC (product code 0113)
•	GRCC David Jones Platinum (product code 0143)
•	0204 (cell 05) – Airmiles Blue
•	0210 (cell 06) – Airmiles Platinum
•	0260 (cell 12) – Airmiles Reserve


```{r}
# to_remove <-  c(113, 143, 204, 210, 260)
# 
# # Get PML products that we need to remove
# to_remove_PML <- PML %>% 
#   filter(NA_Product_Code %in% to_remove)
# 
# # Remove them from the PML
# PML <- PML %>% filter(!NA_Product_Code %in% to_remove)
# 
# # Identify samples to remove (and store there)
# to_remove_Sample <-  MVI_Sample %>% inner_join(to_remove_PML)
#
# # Remove from the Sample
# MVI_Sample <- MVI_Sample %>% anti_join(to_remove_Sample) 
```

### Updating PCT Codes that were misaligned

#### First - General Misalignment

This is misalignment that has occurred for multiple waves and can be fixed before doing the sample prep
```{r}
MVI_Sample <- MVI_Sample %>% 
  mutate(Product_Code = case_when(
    NA_Country_Code == 38 & NA_Cell_Number == 1 & Product_Code == "631" ~ "A6I",
    NA_Country_Code == 38 & NA_Cell_Number == 4 & Product_Code == "631" ~ "EC",
    NA_Country_Code == 38 & NA_Cell_Number == 5 & Product_Code == "630" ~ "EB",
    T ~ Product_Code
  ))
```


#### Second - Quarter Specific Misalignment

NOTE YOU WILL ONLY KNOW IF YOU NEED TO DO THIS AFTER YOU SEND THE FIRST DIAGNOSTIC EMAIL HALFWAY DOWN THE SCRIPT. 

 - This occurs after we identified misalignment further down below, the code had to be adjusted up here so that we extracted the correct subject line inserts and card product names with the adjustments.
  - The information you need is the Unique_Record_Identifier15, Update_NA_Cell, Update_Product,	Update_NA_PCT_Code for all people to update.
 
```{r}
# pct_changes <- read_excel(HELPER_FILE_PATH, 'PCT_Code_Changes') %>%
#   select(Unique_Record_Identifier15, Update_NA_Cell,Update_Product,	Update_PCT_Code) %>%
#   mutate(Update_Cell = str_pad(Update_NA_Cell, 2, 'left', '0'))
# 
# MVI_Sample <- MVI_Sample %>%
#   left_join(pct_changes) %>%
#   mutate(Cell_Number = if_else(!is.na(Update_Cell), Update_Cell, Cell_Number),
#          NA_Cell_Number = if_else(!is.na(Update_NA_Cell), Update_NA_Cell, NA_Cell_Number),
#          Card_Product = if_else(!is.na(Update_Product), Update_Product, Card_Product),
#          Product_Code = if_else(!is.na(Update_PCT_Code), Update_PCT_Code, Product_Code)) %>%
#   select(-starts_with("Update"))
```


### Summary tables
These are used for diangostics and sent to OPS and PM later on

```{r Set up Summary Tables}
# Set up Cell Level summary table
# This is sent to OPs in a few chunks
cell_lvl_summary <- PML %>% 
  select(NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number, Sample_Requested) %>%
  #  Count the number of samples received for each market
  left_join(MVI_Sample %>% 
             count(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext,
                   name = "Sample_Recieved")) %>% 
  # Compute differences
  mutate(Dif = Sample_Recieved - Sample_Requested,
         Pct_dif = round(abs(Dif / Sample_Requested) * 100, 2))

# Create the market level summary table. This is added to at the end of the script
# Will be sent to OPs at the end of the prep process
mrkt_level_summary <- cell_lvl_summary %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Sample_Requested = sum(Sample_Requested, na.rm = T),
            Sample_Recieved = sum(Sample_Recieved, na.rm = T),
            Dif = sum(Dif, na.rm = T))
```

### Checking for Unique ID Duplicates

```{r}
# Check for duplicates within and across country codes
# This has never happened in the past, so if any warnings are shown, it means the raw data is wrong
dups_within_country_code <- MVI_Sample %>% 
  group_by(NA_Country_Code, Unique_Record_Identifier15) %>% 
  filter(n() > 1)

dups_across_country_code <- MVI_Sample[duplicated(MVI_Sample$Unique_Record_Identifier15),]

if (nrow(dups_within_country_code) > 0) {
  warning("Duplicate Unique Record ID's Found within country codes")
  warning("THIS IS A BIG DEAL AND MEANS SOMETHING IS WRONG WITH THE DATA")
  dups_within_country_code$Unique_Record_Identifier15 %>% unique() %>% print()
} else {
  message("SUCCESS: No duplciate Unique Record ID's Found within country codes")
}

if (nrow(dups_across_country_code) > 0) {
  warning("Duplicate Unique Record ID's Found across country codes")
  warning("THIS IS A BIG DEAL AND MEANS SOMETHING IS WRONG WITH THE DATA")
  dups_across_country_code$Unique_Record_Identifier15 %>% unique() %>% print()
} else {
  message("SUCCESS: No duplciate Unique Record ID's Found across country codes")
}
```

### Calculating Bad Spend Observations before deduplication
These are once again just diagnostics for OPs and PM
```{r Spend Checks Pre-Duplicates}
missing_spend_predup <- MVI_Sample %>% filter(is.na(Total_OD_Spend_USD)) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code, 
         NA_Language_Index, File_Ext, Cell_Number)

zero_spend_predup <- MVI_Sample %>% filter(Total_OD_Spend_USD == 0) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code,
         NA_Language_Index, File_Ext, Cell_Number)

neg_spend_predup <- MVI_Sample %>% filter(Total_OD_Spend_USD < 0) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code,
         NA_Language_Index, File_Ext, Cell_Number)
```

```{r}
# Bad spend summary table by cell number
bad_spends_predup <- MVI_Sample %>% 
  group_by(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext) %>% 
  summarize(
    noSpend = sum(is.na(Total_OD_Spend_USD)),
    zeroSpend = sum(Total_OD_Spend_USD == 0, na.rm = T),
    NegativeSpend = sum(Total_OD_Spend_USD < 0, na.rm = T)
  ) %>% 
  arrange(NA_Country_Code, NA_Language_Index, File_Ext) 

# Add bad spending to cell level summary
cell_lvl_summary <- cell_lvl_summary %>% left_join(bad_spends_predup)

# Write to excel to eventually send to OPS Manager
addWorksheet(Diagnostic_WB, "Cell_Level_Summary")
writeData(Diagnostic_WB, "Cell_Level_Summary", cell_lvl_summary)
```

```{r}
# Create a summary table of Spend in USD and LOC to send to OPs
spend_summary <- MVI_Sample %>%
  mutate(Total_OD_Spend_LOC = as.numeric(Total_OD_Spend_Local_Currency)) %>% 
  group_by(Country, NA_Country_Code) %>%
  summarize(across(
    c(Total_OD_Spend_USD, Total_OD_Spend_LOC),
    .fns = list(
      n_obs =  ~length(.),
      n =      ~sum(!is.na(.)),
      nmiss =  ~sum(is.na(.)),
      mean =   ~mean(., na.rm = TRUE),
      median = ~median(., na.rm = TRUE)
    )
  )) %>% ungroup() %>% 
  arrange(NA_Country_Code)

# Write to excel to eventually send to OPS Manager
addWorksheet(Diagnostic_WB, "Spend_Summary (Pre-Removals)")
writeData(Diagnostic_WB, "Spend_Summary (Pre-Removals)", spend_summary)
```

### Tenure Check and Removal

```{r Tenure Removal}
tenure_to_remove <- MVI_Sample %>% 
  # 1 = "Australia, 39 = "New Zealand"
  filter((Tenure < 6 & !NA_Country_Code %in% c(1,39)) | # Tenure less than 6 not in Aus, NZ
         (Tenure < 12 & NA_Country_Code %in% c(1,39)) | # Tenure less than 12 in AUS, NZ
          is.na(Tenure))                                # Missing Tenure

# Store in frequency format for comparison table
# This is added at the end of the script
wrong_tenure_freq <- tenure_to_remove %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Tenure_Removed = n())

# Remove wrong tenures
MVI_Sample <- MVI_Sample %>% 
  filter(!Unique_Record_Identifier15 %in% tenure_to_remove$Unique_Record_Identifier15)

if (nrow(tenure_to_remove) > 0){
  warning(f_str("There were {nrow(tenure_to_remove)} observations removed with erroneous tenure. These are found in the Diagnostic_Files/Removed_Tenure.csv file"))
  
  # Write to excel to eventually send to OPS Manager
  addWorksheet(Diagnostic_WB, "Removed_Tenure_Individuals")
  writeData(Diagnostic_WB, "Removed_Tenure_Individuals", tenure_to_remove)
  
  addWorksheet(Diagnostic_WB, "Removed_Tenure_Summary")
  writeData(Diagnostic_WB, "Removed_Tenure_Summary", wrong_tenure_freq)
  
} else{
  message("No observations removed because of erroneous tenure")
}
```


```{r}
# Adding PML data to the sample
MVI_Sample <- MVI_Sample %>% left_join(PML)

# Checking cell numbers in the PML and sample
cell_no_not_in_pml <- MVI_Sample %>% anti_join(PML) %>% 
  distinct(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext, NA_Cell_Number)

cell_no_not_in_data <- PML %>% anti_join(MVI_Sample) %>% 
  distinct(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext, NA_Cell_Number)

if (nrow(cell_no_not_in_pml) > 0){
  warning("There are Cell Numbers in the sample data not found in the PML")
  cell_no_not_in_pml %>% print()
} else{
  message("SUCCESS: All Cell Numbers in the sample data found in the PML")
}

if (nrow(cell_no_not_in_data) > 0){
  warning("There are Cell Numbers in the PML not found in the sample data")
  cell_no_not_in_data %>% print()
} else{
  message("SUCCESS: All Cell Numbers in the PML found in the sample data")
}
```

### Variable Checks

```{r Variable Checks}
# Verify all values are as expected for each variable
var_checks <- MVI_Sample %>%
  summarise(
    Amex_ID =             all(nchar(Amex_Customer_ID) == 12),
    NA_Country_Code =     all(NA_Country_Code %in% 1:50),
    NA_Language_Index =   all(NA_Language_Index %in% 1:2),
    NA_Cell_Number =      all(NA_Cell_Number %in% 1:25),
    File_Ext =            all(File_Ext %in% c(NA, 1, 2)),
    NA_Product_Code =     all(NA_Product_Code %in% 101:5099),
    Product_Type =        all(Product_Type %in% c("RCP", "GRCC")),
    COB =                 all(COB %in% c("Y", "N")),
    Key_Prod =            all(Key_Prod %in% c(NA, "K")),
    Fee =                 all(Fee %in% c("Y", "N")),
    PML_Rewards =         all(PML_Rewards %in% c("Y", "N")),
    PML_CashBack =        all(PML_CashBack %in% c("Y", "N")),
    PML_MR =              all(PML_MR %in% c("Y", "N")),
    CV_Reporting_Names =  all(str_detect(CV_Reporting_Names, "^[\\w \\p{P}]+$")) # Can have any letters, digits, underscore, space, or punctuation mark
  ) %>% 
  gather(key = "Param", value = "Value") %>% 
  filter(!Value)

if (nrow(var_checks) > 0){
  warning("Variable found with wrong values:")
  message(var_checks$Param)
} else{
  message("SUCCESS: All variables are as expected")
}
```

```{r Missing Language}
# Only FR-CAN is missing so it can be changed to 2
# This is from SAS, the if statement will determine if another language has a missing language
# This was also originally done in Step 4, but it fits better up here
MVI_Sample <- MVI_Sample %>% 
  mutate(Language = if_else(is.na(Language) & Language_Code == "FR-CA", 
                            "2", Language))

missing_language <- MVI_Sample %>% 
  filter(is.na(Language)) %>% pull(Language_Code) %>% unique()

if (length(missing_language) > 0){
  warning(paste0("These Language codes have missing values: ",
                 paste(missing_language, collapse = ", ")))
}
```

### Misaligned PCT Codes

Identifying PCT Codes that are misaligned with the AIF files

```{r Load in PCT, message =F}
# Load in pct list
aif_pct <- read_csv(PCT_LIST_PATH, col_types = 'cccccccc') %>% 
  set_names(c('Cell_Number', 'NA_Country_Code', 'Product_Code', 'Formatted_PCT_Code', 
              'Card_ProductAIF', 'COUNTRY', 'NA_Prd_Code'))  %>% 
  mutate(Formatted_PCT_Code = substr(Formatted_PCT_Code, 1, 3), # Some PCT Codes come in badly formatted with a bunch of extra characters
         Cell_Number = str_pad(Cell_Number, 2, pad = "0"),
         NA_Prd_Code = str_pad(NA_Prd_Code, 4, pad = "0"),
         NA_Country_Code = as.numeric(NA_Country_Code)) %>% 
  distinct()
```

```{r Misaligned PCTs}
# We first check misaligned NA Product Codes
aif_prod_codes <- aif_pct %>% distinct(Cell_Number, NA_Country_Code, NA_Prd_Code)

# Check Product Code
# This is also used to check PCT Codes later
MVI_with_prod_codes <- MVI_Sample %>% 
  select(Unique_Record_Identifier15, Cell_Number, NA_Country_Code, 
         NA_Language_Index, NA_Product_Code, Product_Code) %>% 
  mutate(NA_Product_Code = str_pad(NA_Product_Code, 4, pad = "0")) %>% # Reformat so both files have same format
  left_join(aif_prod_codes, by = c("Cell_Number", "NA_Country_Code")) %>% 
  mutate(dif = NA_Product_Code != NA_Prd_Code) # Identify differences

na_prod_code_misalignment <- MVI_with_prod_codes %>% filter(dif)

if (nrow(na_prod_code_misalignment) > 0){
  warning("There is NA Product Code misalignment with AIF")
  warning("Check the na_prod_code_misalignment variable")
  
} else{
  message("SUCCESS: No NA Product Code misalignment with AIF")
}

# Check PCT Codes
misaligned_pct_codes <- MVI_with_prod_codes %>% 
  select(Unique_Record_Identifier15, Cell_Number, NA_Country_Code, 
         NA_Language_Index, NA_Product_Code, Product_Code) %>% 
  arrange(NA_Country_Code, Cell_Number, Product_Code) %>% 
  # Only keep samples that are not in the aif, meaning they are misaligned
  anti_join(aif_pct, by = c("Cell_Number", "NA_Country_Code", "Product_Code" = "Formatted_PCT_Code"))

n_misaligned <- nrow(misaligned_pct_codes)
if (n_misaligned > 0){
  warning(f_str("There are {n_misaligned} observations with PCT Code misaligned with AIF"))
  
  print(misaligned_pct_codes)
  
  # Save to the output workbook
  addWorksheet(Diagnostic_WB, "PCT_not_in_AIF_Individuals")
  writeData(Diagnostic_WB, "PCT_not_in_AIF_Individuals", misaligned_pct_codes)
  
} else{
  message("SUCCESS: No PCT Code misalignment with AIF")
}
```

```{r }
# There's probably a better way to display this information than the current system
# The design was taken straight from the SAS output


# Summarize misalignment by product code
misaligned_pct_codes_sum <- misaligned_pct_codes %>% select(-Unique_Record_Identifier15) %>% 
  group_by(NA_Country_Code, NA_Language_Index, Cell_Number, Product_Code) %>% 
  summarize(Count = n()) %>% ungroup()

# Get every pct code for a given NA product code
# Some NA product Codes can have up to 80 different PCT Codes
pct_code_spread <- aif_pct %>% select(-Product_Code) %>% 
  group_by(NA_Country_Code, Cell_Number, NA_Prd_Code) %>% 
  mutate(Card_ProductAIF = paste(unique(Card_ProductAIF), collapse = "/")) %>% 
  ungroup() %>% 
  group_by(NA_Country_Code, 
           Cell_Number,
           NA_Prd_Code,
           Card_ProductAIF) %>% 
  mutate(ord = paste0("PCT_Code_", row_number())) %>% 
  ungroup() %>% 
  spread(key = ord, value = Formatted_PCT_Code)

# Get the variable names (PCT_Code_1 - PCT_Code_77) for the purpose of ordering
pcts <- names(pct_code_spread)[str_detect(names(pct_code_spread), "PCT_Code_[0-9]")]

# Append AIF pct codes to misaligned summary table
# This is sent to OPS to tell us what to do with the misalignment
misaligned_pct_codes_sum <- misaligned_pct_codes_sum %>% 
  left_join(pct_code_spread) %>% 
  select(NA_Country_Code, Cell_Number, 
         "PML NA_Product_Code" = NA_Prd_Code,
         Sample_PCT_Code = Product_Code,
         Card_ProductAIF, Count,
         mixedsort(pcts)) # This orders the pct_codes properly

# Save to the output workbook
addWorksheet(Diagnostic_WB, "PCT_not_in_AIF_Summary")
writeData(Diagnostic_WB, "PCT_not_in_AIF_Summary", misaligned_pct_codes_sum) 
saveWorkbook(Diagnostic_WB, file = f_str("../Files_to_send/MVI_Diagnostics_{MVIQ}-{Sys.Date()}.xlsx"), overwrite = TRUE)
```

### STOP HERE: SEND FILES TO OPS {style="color: red"}

Send the following file to OPS
  - `MVI_Diagnositcs_QUARTER.xlsx` found in the `Files_to_send` folder
  
But, instead of attaching files, copy the file to the `\\pm1\33-626\Quantitative\Sampling-Weighting\Communications\QUARTER` folder and send the link. Here is an example email.

    Hi Allie,
    
    Here is a diagnostic file with some initial findings from the data for you to review. Please let us know what you think: `\\pm1\33-626\Quantitative\Sampling-Weighting\Communications\QUARTER\MVI_Diagnositcs_QUARTER.xlsx`.
    
    The file contains 6 sheets.
      - Cell_Level Summary: Summary of sample received vs sample requested by cell number
              - Nothing appears to be too alarming in the differences between requested and recieved samples. 
              - The largest percent differences occur in markets where all cells recieved the same amount of extra sample. For example, every cell in Belgium recieved 9 more samples than requested, which resulted in a 40% difference in one cell between requested and recieved.
    
    -  Spend_Summary (Pre-Removals): Spend summary statistics prior to removing any observations
    
    - Removed_Tenure_Individuals & Removed_Tenure_Summary: Individuals removed for low tenure, which is <12 in Australia and New Zealand and <6 elsewhere, and summary by market.
              - There were 529 people removed, all from Belgium (this has happened the past 3 quarters for Belgium).
    
    THIS IS WHERE WE NEED ACTION TO MOVE ON TO THE NEXT STEPS.
    
    - PCT_not_in_AIF_Individuals & PCT_not_in_AIF_Summary: PCT Codes from the PML that do not line up with the AIF (based on country code, cell number, and NA product code)
        - Therer are only 4 instances this quarter.
        - We need action taken in the summary file.
        -	The column “Sample_PCT_Code” is the code from the sample data that was not found in the AIF file. The columns PCT_Code_1 through PCT_Code_80 are the PCT codes in the AIF file for the given NA Product Code.
        -	Let us know what you want us to do with the observations with these wrong PCT codes (remove, change, or something else).
    
    Let me know if you have any questions.

#### **When PM gets back on which to remove, update the next chunk**


  - If they say you can remove all samples then just proceed with running the next chunk
  
  
  - If they want you to update the values, then follow these steps
    1. Create a new sheet in the helper file called "PCT_Code_Changes" and add 4 columns
        - Unique_Record_Identifier15, Update_NA_Cell,Update_Product,	Update_NA_Product_Code
        - Ideally the PM/OPS provides the information to you in this format already
        - It's fine if there's more columns, just make sure we have the 4 listed above
    2. Go back up to the top of the code to the 'Load in the data' chunk, and just run the single line that loads in the mvi_sample from the temporary data folder. This resets the data to the original state before we removed anything or made changes.
    3. Uncomment the code chunk under the header "Updating PCT Codes that were misaligned"
    4. Rerun everything under the loading in, and when you get to pct code misaligned, nothing should be misaligned anymore. 
    5. You do not need to resend the email of diagnostics. The diagnostic file will be updated, and will be sent at the end of the process. 
    6. Then run the chunk below. If all went well, `misaligned_pct_codes` will be empty and nothing will change to the database
      - We still need to run this chunk because we need the removed_pcts_ct for the summary at the end (it will just be zeros unless they wanted you to remove some on top of making changes)

```{r}
# Default to removing all
pct_codes_to_remove <- misaligned_pct_codes 

# In the following, add filter statements to remove PCT codes that the PM said we should NOT delete
# So the misaligned pct codes with country, cell, and product codes of (X1, Y1, Z1) and (X2, Y2, Z2) would not be removed in the below example. They are removed from the pct_codes_to_remove dataframe so they are not removed from the sample
# The pct_codes_to_remove should only include what we want to remove

# pct_codes_to_remove <- pct_codes_to_remove %>% 
#   filter(!(NA_Country_Code == X1 & Cell_Number == Y1 & Product_Code == Z1)) %>%
#   filter(!(NA_Country_Code == X2 & Cell_Number == Y2 & Product_Code == Z2)) %>%

n_before <- nrow(MVI_Sample)

MVI_Sample <- MVI_Sample %>% 
  anti_join(pct_codes_to_remove, by = "Unique_Record_Identifier15")

message(f_str("There were {n_before - nrow(MVI_Sample)} samples removed with erroneous PCT Codes"))

# For Summary table at the end
removed_pcts_ct <- pct_codes_to_remove %>% 
  group_by(NA_Country_Code, NA_Language_Index) %>% 
  summarize(Misaligned_PCTs = n())
```

```{r}
rm(MVI_with_prod_codes)
```

### Duplicate Removal

Identifying and removing duplicated AMEX IDs

```{r}
amex_id_dups <- MVI_Sample %>% 
  group_by(Amex_Customer_ID) %>% 
  filter(n() > 1) 

amex_id_dups_within_country <- MVI_Sample %>% 
  group_by(NA_Country_Code, Amex_Customer_ID) %>% 
  filter(n() > 1) 

amex_id_dups_across_country <- amex_id_dups %>% anti_join(amex_id_dups_within_country, by = "Unique_Record_Identifier15")

n_dups <- n_distinct(amex_id_dups$Amex_Customer_ID)
n_dups_across <- n_distinct(amex_id_dups_across_country$Amex_Customer_ID)

message(f_str("There are {n_dups} duplicate Amex Customer IDs, leading to {nrow(amex_id_dups) - n_dups} records being removed."))

if (n_dups_across > 0){
  warning(f_str("Of these, there are {n_dups_across} duplicate Amex Customer IDs across countries, with {nrow(amex_id_dups_across_country)} total duplicate records. Please consult the PM about what to do about these."))
  warning('Ask them now. Copy the file `../Files_to_Send/Amex_ID_dups_across_country.csv` to the communications folder and ask them in an email.')
  
  amex_id_dups_across_country %>% 
    select(NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number, Amex_Customer_ID, Unique_Record_Identifier15, Tenure, Total_OD_Spend_USD) %>% 
    write_csv("../Files_to_Send/Amex_ID_dups_across_country.csv")
}
```


```{r}
amex_dups_summary <- amex_id_dups %>% 
  freq_table(c("NA_Country_Code", "NA_Language_Index", "File_Ext"))

# Write to excel to send to OPS Manager
addWorksheet(Diagnostic_WB, "AMEXID_Dupes_Inds")
writeData(Diagnostic_WB, "AMEXID_Dupes_Inds", amex_id_dups)

addWorksheet(Diagnostic_WB, "AMEXID_Dupes_Summary")
writeData(Diagnostic_WB, "AMEXID_Dupes_Summary", amex_dups_summary)
```

```{r}
# Sort data by na_cell_number in order to keep the dup record with the higher priority - 
  # in this case, the number closest to 1. i.e 1>2>3>4>5...;
# If there are dupes with the same cell number, just keep any one record - doesn't matter b/c spend is the same for all dupes;
MVI_Sample_no_dups <-  MVI_Sample %>% 
  arrange(NA_Country_Code, Cell_Number) %>% 
  group_by(NA_Country_Code, Amex_Customer_ID) %>% 
  filter(row_number() == 1) %>% ungroup()
```

```{r}
########### DUPLICATES ACROSS COUNTRY
# THIS IS NOT SET IN STONE
# COULD BE WE WANT THE ONES THAT DONT HAVE ENOUGH SAMPLE
# THIS IS FOR THE PM/OPS TO DECIDE IF WE HAVE DUPLICATES ACROSS COUNTRY

 # MVI_Sample_no_dups <- MVI_Sample_no_dups %>% 
 #  # We want the first occurrence by cell number
 #  arrange(Cell_Number) %>% 
 #  group_by(Amex_Customer_ID) %>% 
 #  filter(row_number() == 1) %>% ungroup() %>% 
 #  # Reset ordering
 #  arrange(NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number)
```

```{r Removed Dupes}
removed_dups <- anti_join(amex_id_dups, MVI_Sample_no_dups, by = "Unique_Record_Identifier15")

removed_dups_summary <- removed_dups %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Amex_ID_dupes = n())

# Write to excel to send to OPS Manager
addWorksheet(Diagnostic_WB, "Rem_AMEXID_Dupes_Inds")
writeData(Diagnostic_WB, "Rem_AMEXID_Dupes_Inds", removed_dups)

addWorksheet(Diagnostic_WB, "Rem_AMEXID_Dupes_Summary")
writeData(Diagnostic_WB, "Rem_AMEXID_Dupes_Summary", removed_dups_summary)
```

```{r}
# Clear some memory
rm(MVI_Sample)
```


### Creating New Variables


```{r Create new variables, message=F, warning=F}
# Basically what's going on in the next few lines is we create a dataframe with all of the unique product codes in the sample dataset and then assign values for each of the created variables based on the conditions in the CV_Product_Codes sheet.
# We then join this to original dataframe by product code. 

# Load in new variable rules
cv_product_codes <- read_excel(HELPER_FILE_PATH, sheet = "CV_Product_Codes")

# Create a table of product codes
cv_vars_tab <- MVI_Sample_no_dups %>% distinct(NA_Product_Code)

# Append the new variables to the product code table based on the PM's instructions
for (var in CV_VARS) cv_vars_tab <- cv_vars_tab %>% add_cv_var(var, cv_product_codes)

# Apply the new variables to sample
MVI_Sample_new_vars <- MVI_Sample_no_dups %>% left_join(cv_vars_tab)
```

```{r Create new variables 2}
MVI_Sample_new_vars <- MVI_Sample_new_vars %>% 
  mutate(
    CV_COBRAND_AIRLINES = case_when(
    	NA_Product_Code %in% c('3556', '3570') ~ "",
    	CV_COBRAND == "TRUE" & CV_AIRLINE == "TRUE" ~ "Yes",
    	CV_COBRAND == "TRUE" & CV_AIRLINE == "FALSE" ~ "No",
    	TRUE ~ ""),
    
    CV_Key_Products = if_else(is.na(Key_Prod), "N", "Y"), # This was originally Key_Prod == "K" but it doesn't account for missing, and there's no other values anyway
    
    CV_Product_Type = case_when(
      Product_Type == "RCP" & COB == "N" ~ "RCP",
      Product_Type == "GRCC" & COB == "N" ~ "GRCC",
      COB == "Y" ~ "Co-Brand"),
    
    CV_PREMIUM = CV_CENTURION == "TRUE" | CV_PLATINUM == "TRUE",
    
    CV_INTERVIEW_DATE = paste0(YEAR, MONTH),
    
    CV_REPORTING_NAME = CV_Reporting_Names,
    
    Date_of_Birth = str_replace_all(Date_of_Birth, "-", "/"), # Some date are different format
    dYOB = as.Date(Date_of_Birth) %>% year(),
    
    GENERATION = case_when(is.na(dYOB) | 
                             dYOB < 1850 | dYOB > (YEAR - 17) ~ "",
                           dYOB <  1946 ~ "Silent: 1945 and prior",
                           dYOB <  1965 ~ "Baby Boomers: 1946 - 1964",
                           dYOB <  1980 ~ "Generation X: 1965 - 1979",
                           dYOB <  1989 ~ "Older Millennials: 1980 - 1988",
                           dYOB <  1997 ~ "Younger Millennials: 1989 - 1996",
                           dYOB >= 1997 ~ "Generation Z: 1997 and later"),

  Email = paste0(Unique_Record_Identifier15, "@amexgabmsurvey.com") # Why are we doing this, when we do not include it in the output?
         )
```

#### Remove people with wrong member since or DOB

```{r Member Since and DOB flags}
# THIS MAY NEED TO BE CHANGED. IF IT'S QUARTER 1, WE SHOULD BE LOOKING AT Member_Since > (YEAR - 1)
member_since_flag <- MVI_Sample_new_vars %>% 
  filter(Member_Since < 1920 | Member_Since > YEAR)

dob_flag <- MVI_Sample_new_vars %>% 
  filter(dYOB < 1850 | dYOB > YEAR - 17)

# Remove flagged observations
MVI_Sample_new_vars <- MVI_Sample_new_vars %>% 
  filter(!Unique_Record_Identifier15 %in% 
           c(member_since_flag$Unique_Record_Identifier15, 
             dob_flag$Unique_Record_Identifier15))

# Get counts for summary table
ms_ct <- member_since_flag  %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Member_Since = n())

dob_ct <- dob_flag %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(YOB = n())

removed_ms_dob <- member_since_flag %>% 
  bind_rows(dob_flag) %>% 
  select(NA_Country_Code, NA_Language_Index, File_Ext, Amex_Customer_ID, Unique_Record_Identifier15, Member_Since, dYOB, Tenure)

# Write to excel to send to OPS Manager
addWorksheet(Diagnostic_WB, "Removed_Member_Since_DOB")
writeData(Diagnostic_WB, "Removed_Member_Since_DOB", removed_ms_dob)
```


```{r Final Sample Counts}
# Final counts for market level summary
final_counts <- MVI_Sample_new_vars %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(Final_Sample = n())
```

```{r}
# Clear up memory
rm(MVI_Sample_no_dups)
```

### Weighting Segments

```{r Weighting Segments}
# Weighting Segments
# The weighting segment sheet doesn't include XS or XT, 
# So to make it easier, I'm just having one line for each segment with it's respective tenure and spend splits
# Then we create the 6 conditions for the various options given these splits
weighting_segments <- read_excel(HELPER_FILE_PATH, sheet = "Weighting_Segments") %>%
  filter(str_detect(WGT_Bucket, "MVI")) %>% # Removes SBS weights if they weren't removed in the helper file
  filter(str_detect(Tenure_Split, "\\+") & str_detect(Spend_Split, "\\+")) %>% # HTHS
  # Turn rows with multiple NA_Product_Codes into their own row to allow for joining with MVI data
  separate_rows(NA_Product_Code, sep = ", ") %>% 
  mutate(NA_Product_Code = trimws(NA_Product_Code), # just in case there were extra spaces
         pc_cond = paste0("NA_Product_Code == ", NA_Product_Code),
         # Turn {QUARTER}-9{SEGMENT} into {QUARTER}{SEGMENT}
         WV_Weighting_Segment = str_remove(WGT_Bucket, "-[0-9]"), 
         # Extract numeric values from tenure and spend conditions
         Tenure_Split = parse_number(Tenure_Split),
         Spend_Split = parse_number(Spend_Split) * 1000,
         )
  
# Create Different conditions for each segment
MVI_Sample_ws <- MVI_Sample_new_vars %>% left_join(weighting_segments) %>% 
  mutate(Tenure_Bucket = case_when(Tenure >= 6 & Tenure < Tenure_Split ~ "LT",
                                   Tenure >= Tenure_Split ~ "HT",
                                   TRUE ~ "XT"), # This can't occur as Tenure <6 is removed above
         Spend_Bucket  = case_when(Total_OD_Spend_USD > 0 & Total_OD_Spend_USD < Spend_Split ~ "LS",
                                   Total_OD_Spend_USD >= Spend_Split ~ "HS",
                                   TRUE ~ "XS"),
         
         # Create the weighting segment
         Weighting_Segment = if_else(Spend_Bucket == "XS",
                                     "MVINOSPEND",
                                     # Simple as combine the segment name, tenure bucket, and spend bucket (ie, MVIQ3230101 + HT + HS = MVIQ3230101HTHS)
                                     paste0(WV_Weighting_Segment, Tenure_Bucket, Spend_Bucket))) %>% 
  select(-ends_with("Bucket"), everything())
```

```{r}
# Table showing observed minimums and maximums for tenure and spend for each weighting segment
ws_table <- MVI_Sample_ws %>% group_by(Weighting_Segment, Tenure_Split, Spend_Split) %>%  
  summarize(min_t = min(Tenure),
            max_t = max(Tenure),
            min_s = min(Total_OD_Spend_USD),
            max_s = max(Total_OD_Spend_USD)) %>% 
  make_nice_table("Weighting Segments")
```

```{r}
# Clear up memory
rm(MVI_Sample_new_vars)
```

```{r Scientific Notation}
# Convert product codes with 2nd character as E and 1st and 3rd characters 0-9. We need to create a temp product code variable which recodes the 2rd character from E to # in order to prevent excel from changing the product code to scientific notation. must recode the # back to E in excel;

# CAN WE REMOVE THIS BECAUSE WE ARE DIRECTLY WRITING TO CSVs AND NOT USING EXCEL
sci_no_to_fix <- MVI_Sample_ws %>% 
  distinct(NA_Country_Code, Product_Code) %>% 
  filter(str_detect(Product_Code, "[0-9]E[0-9]"))

MVI_Sample_ws <- MVI_Sample_ws %>% 
  mutate(Product_Code = ifelse(str_detect(Product_Code, "[0-9]E[0-9]"),
                               gsub("E", "#", Product_Code),
                               Product_Code))
```


#### Calculating Bad Spend Observations after Deduplication

```{r Spend Checks After-Duplicates}
missing_spend <- MVI_Sample_ws %>% filter(is.na(Total_OD_Spend_USD)) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number)

zero_spend <- MVI_Sample_ws %>% filter(Total_OD_Spend_USD == 0) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number)

neg_spend <- MVI_Sample_ws %>% filter(Total_OD_Spend_USD < 0) %>% 
  select(Amex_Customer_ID, Unique_Record_Identifier15, NA_Country_Code, NA_Language_Index, File_Ext, Cell_Number)

# Write to csv
# These aren't sent anywhere but are kept just in case
missing_spend %>% write_csv(f_str("../Diagnostic_Files/Missing_Spend_{MVIQ}.csv"), na = "")
zero_spend %>% write_csv(f_str("../Diagnostic_Files/Zero_Spend_{MVIQ}.csv"), na = "")
neg_spend %>% write_csv(f_str("../Diagnostic_Files/Negative_Spend_{MVIQ}.csv"), na = "")
```

```{r}
# Bad spend summary table by cell number
bad_spends <- MVI_Sample_ws %>% 
  group_by(NA_Country_Code, NA_Language_Index, Cell_Number, File_Ext) %>% 
  summarize(
    noSpend = sum(is.na(Total_OD_Spend_USD)),
    zeroSpend = sum(Total_OD_Spend_USD == 0, na.rm = T),
    NegativeSpend = sum(Total_OD_Spend_USD < 0, na.rm = T)
  ) %>% 
  arrange(NA_Country_Code, NA_Language_Index, File_Ext) %>% ungroup()

bad_spends_countrylevel <- bad_spends %>% 
  select(-Cell_Number) %>% 
  group_by(NA_Country_Code, NA_Language_Index, File_Ext) %>% 
  summarize(across(.fns = ~sum(., na.rm = T)))
```

#### Create Market Level Summary Table

```{r}
# This contains all the summary tables we created through the code
mrkt_level_summary <- mrkt_level_summary %>% 
  left_join(final_counts) %>% 
  left_join(dob_ct) %>% 
  left_join(ms_ct) %>% 
  left_join(wrong_tenure_freq) %>% 
  left_join(removed_pcts_ct) %>%
  left_join(removed_dups_summary) %>% 
  left_join(bad_spends_countrylevel) %>% 
  adorn_totals('row')

# Write to excel to send to OPS Manager
addWorksheet(Diagnostic_WB, "Market_lvl_Summary")
writeData(Diagnostic_WB, "Market_lvl_Summary", mrkt_level_summary)
saveWorkbook(Diagnostic_WB, file = f_str("../Files_to_send/{MVIQ}_Diagnostics_{Sys.Date()}.xlsx"), overwrite = TRUE)
```

### Reformatting to match what Qualtrics is expecting

```{r}
# Change all date columns to "M/D/YYYY" format
MVI_Sample_ws <- MVI_Sample_ws %>% 
  mutate(across(c("Establishment_Date"), ~str_replace_all(., "-", "/")), # Some date are different format
         across(c("Establishment_Date"), ~format(as.Date(.x), format = "%m/%d/%Y")),
         # We need to remove leading zeros in months and days
         across(c("Establishment_Date"), ~gsub("^0", "", gsub("/0", "/", .))),
         # Make Customer ID 15 digits
         Amex_Customer_ID = str_pad(Amex_Customer_ID, 15, "left", "0"),
         # Make Country Code 2 digits
         NA_Country_Code = str_pad(NA_Country_Code, 2, "left", "0"),
         # Make Product Code 4 digits
         NA_Product_Code = str_pad(NA_Product_Code, 4, "left", "0")
         )
```


### Creating Final Sample

```{r Create Final Sample}
# Add AIF PCT Codes (Old Step 5)
MVI_final_postals <- MVI_Sample_ws %>% left_join(aif_pct %>% mutate(aif = TRUE, 
                                                                    NA_Country_Code = str_pad(NA_Country_Code, 2, "left", "0"))) %>% 
  mutate(Card_Product = if_else(is.na(aif), Card_Product, Card_ProductAIF), # Products not in the AIF get to keep their names
         # These two variables are just blanks
         MYCA_Indicator = "",
         Paperless_Indicator = ""
         ) 

MVI_final <- MVI_final_postals %>% 
  select(Cell_Number,
         COUNTRY_CODE = NA_Country_Code,
         Product_Code,
         Member_Since,
         Establishment_Date,
         Tenure,
         Language_Preference_Code = Language,
         MYCA_Indicator, 
         Paperless_Indicator,
         MR_Member_ID,
         Card_Product, 
         Account_number_last_5_digits,
         Amex_Customer_ID,
         Avg_Overseas_Domestic_ROCs,
         MR_Points,               
         Tot_Overseas_Domest_Spend_LOC = Total_OD_Spend_Local_Currency,
         Tot_Overseas_Domest_Spend_USD = Total_OD_Spend_USD,
         UID = Unique_Record_Identifier15,
         COUNTRY = Country,
         NAXION_PRODUCT_CODE = NA_Product_Code,
         CV_ICS_Region,
         CV_Product_Type,
         CV_Key_Products,
         CV_Fee = Fee,
         CV_PML_Rewards = PML_Rewards,
         CV_PML_CashBack = PML_CashBack,
         CV_PML_FYF,
         CV_PML_WAIVER,
         CV_PML_MR = PML_MR,
         CV_CENTURION,
         CV_PLATINUM,
         CV_PREMIUM,
         CV_REPORTING_NAME,
         CV_COBRAND_AIRLINES,
         CV_RCP_GOLD,
         CV_RCP_GREEN,
         CV_INTERVIEW_DATE,
         LANGUAGE_CODE = Language_Code,
         WV_WEIGHTING_SEGMENT = Weighting_Segment,
         SUBJECT_LINE_INSERTION = Subject_Line_Insertion, 
         GENERATION,
         
         # Remove after checking
         File_Ext,
         Date_of_Birth,
         CV_COBRAND,
         CV_AIRLINE,
         COB,  
         Filename
         )
```

```{r}
# Save the postals and checking db in case you want to close R and not have to rerun everything to write them for Bob after everything is created
# They can be reloaded in down below

save(MVI_final_postals, file=f_str("../Temporary_Data/{MVIQ}_Postals.Rdata"))
save(MVI_final, file=f_str("../Temporary_Data/{MVIQ}_Checking_{Sys.Date()}.Rdata"))
```


```{r}
# Clear up memory
rm(MVI_Sample_ws)
```

```{r Writing Merged File with Checks}
# THIS IS SO OPS CAN VERIFY WHAT WAS DONE INTERNALLY
# This includes extra variables to allow for checking
         # File_Ext,
         # Date_of_Birth,
         # CV_COBRAND,
         # CV_AIRLINE,
         # COB,  
         # Filename
MVI_final %>% write_csv(f_str("../All_Sample_Files/{MVIQ}_Checking_{Sys.Date()}.csv"), na = "")
```

```{r}
# This is if you wanted to close R while waiting for OPS to review everything
# You can just reload in the data without rerunning the whole script

last_save_date <- '2023-10-03' # last time you saved the mvi_final dataset
# load("../Temporary_Data/{MVIQ}_Checking_{last_save_date}.Rdata")
```

### STOP HERE: SEND FILES TO OPS {style="color: red"}

Send the following file to OPS (there are new sheets added)
  - `MVI_Diagnositcs_QUARTER.xlsx` found in the `Files_to_send` folder
  
Once again, save a copy of the file in the `Communications/Quarter` folder and send a link. You can override the old file because the new file has the old data in it. You also are sending the final csv for OPs and the PM to make their checks. 
  - `MVI_Final_Checking-DATE.csv`

Importantly, you need to create an excel workbook from the csv manually and send that. Open the csv, and reformat the numeric variables that need padded zeroes. I usually insert a column at the start of the file (i.e., column A) and Use the TEXT function in excel and use the following zeroes formats. Then copy and paste AS VALUES into the respective columns to overwrite the original data. Once again. This is just for convenience for OPS, this does not affect the data in the official CSV or data. 

DO NOT SAVE THE CSV. USE "SAVE AS" AND MAKE A NEW EXCEL FILE. Also copy this new file to the communications folder

Formats: 
  - Cell_Number : "00" 
  - COUNTRY_CODE : "00"
  - Account_number_last_5_digits: "00000"
  - Amex_Customer_ID : "000000000000000"
  - NAXION_PRODUCT_CODE: "0000"
  

Importantly, the non-english characters in Subject_Line_Insertion will still be messed up in the excel. This is okay, since they are never changed in the csv.

once this is done, copy the csv and xlsx into the `\\pm1\33-626\Quantitative\Sampling-Weighting\All Sample Files\QUARTER` folder.

Here is an example email
    
    Hi,
    
    The final sample has been prepped. Please refer again to the diagnostic file (with new sheets added, and old ones updated): 
    
    There were 6 sheets added.
      - AMEXID_Dupes_Ind & AMEXID_Dupes_Summary: Individuals and summary of all duplicate amex id observations
      - Rem_AMEXID_Dupes_Ind & Rem_AMEXID_Dupes_Summary: Individuals removed for having a duplicate Amex Customer ID. We kept the one observation with the highest priority. 
        - There were 1214 duplicate Amex Customer IDs, leading to 1221 records being removed.
        -	There were no odd duplicates across countries like two quarters ago.
    
      - Removed_Member_Since_DOB: People who were removed for having unlikely dates of birth or member_since values. 
        - This quarter, 5 people were removed because of unlikey member_since values (>2023 or < 1920)
    
      - Market_lvl_Summary: Final summary of all counts and removals that were done by market. It also includes the number of no spend, zero spend, and negative spend per market.
    
    
    Here are the final merged csv and excel for you to perform your checks. Excel messes up some formatting, but the csv contains all the true formatting of variables and is the file we use for the individual CSVs. For example, if you open the .xlsx file or if you open the .csv in excel, the Subject_Line_Insertion will look messed up for non-english characters. This is just an excel issue, the values are fine in the csv. You can double check them in the csv in a text editor like Notepad++. However, We reformatted numeric variables that need padded zeroes for your convenience.
    -	\\pm1\33-626\Quantitative\Sampling-Weighting\All Sample Files\Q3MVI_Final_Checking_MVIQ323.xlsx
    -	Here’s the csv: \\pm1\33-626\Quantitative\Sampling-Weighting\All Sample Files\Q3\MVI_Final_Checking_MVIQ323.csv
    -	Note there are a couple extra variables at the end to help with checking. These are removed later.
             # FileExt,
             # Date_of_Birth,
             # CV_COBRAND,
             # CV_AIRLINE,
             # COB,  
             # Filename
    
    Please let me know if everything is all set to be written to individual market CSV's.


```{r Writing Merged File}
# Writing Official Merged File
# Get rid of checking variables
MVI_final <- MVI_final %>% 
  select(-c(File_Ext,
           Date_of_Birth,
           CV_COBRAND,
           CV_AIRLINE,
           COB))
         
MVI_final %>% select(-Filename) %>% write_csv(f_str("../All_Sample_Files/{MVIQ}_Final_{Sys.Date()}.csv"), na = "")
```

#### Write final files as individual CSVs

```{r Writing Market CSVs}
MVI_final %>%
  group_by(Filename) %>%
  # In group_walk, .x is the individual group and .y is the grouping name
  # So in our case, .x is the dataframe for a given filename, and .y is the filename
  group_walk(~write_csv(.x, file.path(MARKET_FILES_PATH, .y), na = ""))
```

### Postal Codes for Bob Crown

```{r}
# If you closed r and just want to load in the postals db, uncomment this line
# load("../Temporary_Data/{MVIQ}_Postals.Rdata")

postals <- MVI_final_postals %>% 
  select(Amex_Customer_ID,
         Unique_Record_Identifier15,
         NA_Product_Code,
         Country,
         Language_Code,
         Postal_Code)

postals %>% write_csv(f_str("../Files_to_send/{MVIQ}_Postal_Codes_{Sys.Date()}.csv"), na = "")
```
